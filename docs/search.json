[
  {
    "objectID": "setup.html",
    "href": "setup.html",
    "title": "C2C: Setup and Face-up Poker",
    "section": "",
    "text": "You can use a code editor of your choice. We like VSCode. You can also work with Google Colab in the browser. The instructions below are for coding locally on MacOS or Unix.\n\n\nMake a local directory for the project cardstocode. From the command line:\nmkdir cardstocode\ncd cardstocode\n\n\n\nCreate and activate a virtual environment:\npython3 -m venv venv\nsource venv/bin/activate\nYou should now see (venv) at the beginning of your command prompt, indicating that the virtual environment is active.\n\n\n\nCreate a Python file c2c.py. From the command line:\ntouch c2c.py\n\n\n\nTo deactivate:\ndeactivate\nTo reactivate:\nsource venv/bin/activate\n\n\n\nInstall numpy, which we will use later.\npip install numpy",
    "crumbs": [
      "About",
      "Setup and Face-up Poker"
    ]
  },
  {
    "objectID": "setup.html#set-up-your-environment",
    "href": "setup.html#set-up-your-environment",
    "title": "C2C: Setup and Face-up Poker",
    "section": "",
    "text": "You can use a code editor of your choice. We like VSCode. You can also work with Google Colab in the browser. The instructions below are for coding locally on MacOS or Unix.\n\n\nMake a local directory for the project cardstocode. From the command line:\nmkdir cardstocode\ncd cardstocode\n\n\n\nCreate and activate a virtual environment:\npython3 -m venv venv\nsource venv/bin/activate\nYou should now see (venv) at the beginning of your command prompt, indicating that the virtual environment is active.\n\n\n\nCreate a Python file c2c.py. From the command line:\ntouch c2c.py\n\n\n\nTo deactivate:\ndeactivate\nTo reactivate:\nsource venv/bin/activate\n\n\n\nInstall numpy, which we will use later.\npip install numpy",
    "crumbs": [
      "About",
      "Setup and Face-up Poker"
    ]
  },
  {
    "objectID": "setup.html#face-up-1-card-poker",
    "href": "setup.html#face-up-1-card-poker",
    "title": "C2C: Setup and Face-up Poker",
    "section": "Face-up 1-card poker",
    "text": "Face-up 1-card poker\nWe’ll start by setting up a poker game with a 3-card deck where the cards are given values {0, 1, 2}. Each player is dealt a card “face-up” and the higher card wins. This is like the game War, but even more simplified because there are no ties since the entire deck is only 3 cards.\n\nStep 1: Imports\nLet’s start by importing numpy (will be used later) and random.\n\n\n\n\n\n\nImports\n\n\n\n\n\nimport numpy as np\nimport random\n\n\n\n\n\nStep 2: Cards and scores\nDefine a Game class that is initialized with these class attributes:\n\n3-card deck array with cards 0, 1, and 2\nCard placeholders for each player in a cards array that are initialized as None\nScore placeholders for each player in scores array that are initialized as 0\n\n\n\n\n\n\n\nInitialize Game class\n\n\n\n\n\nclass Game:\n    def __init__(self):\n        self.deck = [0, 1, 2]\n        self.cards = [None, None]\n        self.scores = [0, 0]\n\n\n\n\n\nStep 3: Deal cards\nLet’s now add a deal_cards function into the Game class that samples two cards from the deck into self.cards. This is the first of two class methods.\n\n\n\n\n\n\ndeal_cards function\n\n\n\n\n\n    def deal_cards(self):\n        self.cards = random.sample(self.deck, 2)\n\n\n\n\n\nStep 4: Compare cards\nAccumulate the scores for each player using self.scores in a compare_cards function. This is the second class method.\n\n\n\n\n\n\nplay_round function\n\n\n\n\n\n    def compare_cards(self):\n        if self.cards[0] &gt; self.cards[1]:\n            self.scores[0] += 1\n            self.scores[1] -= 1\n        elif self.cards[0] &lt; self.cards[1]:\n            self.scores[0] -= 1\n            self.scores[1] += 1\n\n\n\n\n\nStep 5: Play round\nCreate a play_round function in the Game class that deals the cards using self.deal_cards.\n\n\n\n\n\n\nUpdate play_round function\n\n\n\n\n\n    def play_round(self):\n        self.deal_cards()\n        self.compare_cards()\n\n\n\n\n\nStep 6: Run the game\nCreate the main function that defines num_games as 100, creates an object of the Game class in game, and runs the game num_games times.\n\n\n\n\n\n\nmain function\n\n\n\n\n\ndef main():\n    num_games = 100\n    game = Game()\n\n    for _ in range(num_games):\n        game.play_round()\nThe game object now has all the attributes and methods defined in the Game class.\n\n\n\n\n\nStep 7: Print the scores\nPrint the scores for each player after the game ends, also in the main function.\n\n\n\n\n\n\nPrint scores\n\n\n\n\n\n\n    print(f\"After {num_games} games:\")\n    print(f\"Player 1 score: {game.scores[0]}\")\n    print(f\"Player 2 score: {game.scores[1]}\")\n\n\n\n\n\nStep 8: Create the main block\n\n\n\n\n\n\nMain block\n\n\n\n\n\nif __name__ == \"__main__\":\n    main()\n\n\n\n\n\nPutting it all together\n\n\n\n\n\n\nAll the code\n\n\n\n\n\nimport random\n\nclass Game:\n    def __init__(self):\n        self.deck = [0, 1, 2]\n        self.cards = [None, None]\n        self.scores = [0, 0]\n\n    def deal_cards(self):\n        self.cards = random.sample(self.deck, 2)\n\n    def compare_cards(self): \n        if self.cards[0] &gt; self.cards[1]:\n            self.scores[0] += 1\n            self.scores[1] -= 1\n        elif self.cards[0] &lt; self.cards[1]:\n            self.scores[0] -= 1\n            self.scores[1] += 1\n\n    def play_round(self):\n        self.deal_cards()\n        self.compare_cards()\n\ndef main():\n    num_games = 100\n    game = Game()\n\n    for _ in range(num_games):\n        game.play_round()\n\n    print(f\"After {num_games} games:\")\n    print(f\"Player 1 score: {game.scores[0]}\")\n    print(f\"Player 2 score: {game.scores[1]}\")\n\nif __name__ == \"__main__\":\n    main()\n\n\n\n\n\nRun the code\npython3 c2c.py\nThe output should look something like:\n\n\n\n\n\n\nSample code output from 100 games\n\n\n\n\n\nAfter 100 games:\nPlayer 1 score: -12\nPlayer 2 score: 12",
    "crumbs": [
      "About",
      "Setup and Face-up Poker"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "C2C: About",
    "section": "",
    "text": "We’re excited about learning through games. We developed Cards to Code as a foundational course on building a functional poker bot after running an AI Poker Camp course in San Francisco in summer 2024.\nCheck out Overbet.ai (under development) and Poker Camp.\nWe’ll also have more advanced courses available soon."
  },
  {
    "objectID": "about.html#about-us",
    "href": "about.html#about-us",
    "title": "C2C: About",
    "section": "",
    "text": "We’re excited about learning through games. We developed Cards to Code as a foundational course on building a functional poker bot after running an AI Poker Camp course in San Francisco in summer 2024.\nCheck out Overbet.ai (under development) and Poker Camp.\nWe’ll also have more advanced courses available soon."
  },
  {
    "objectID": "about.html#why-poker",
    "href": "about.html#why-poker",
    "title": "C2C: About",
    "section": "Why poker?",
    "text": "Why poker?\nLet’s split this into two questions.\n\nWhy play poker?\nGames of incomplete information like poker are most similar to real-life settings. Incomplete information means that there is some hidden information – in poker this is the private opponent cards and, in games like Texas Hold’em, the yet to be revealed board cards.\nThis means we need to infer our opponents’ hands and understand the probabilities of the cards that we can’t see.\nPoker is interesting from a math perspective – probability, expected value, risk, and bankroll management are all important skills.\nAlso from a psychological perspective – emotional control, reading opponents, and adapting to other players are all valuable.\nWe learn to make decisions and focus on the quality of the decision, rather than the results.\n\n\nWhy build a poker bot?\nPoker has clear rules and structure, so we can apply reinforcement learning where the rewards are the profits in the game. Reinforcement learning, game theory, and Monte Carlo methods are applicable across many domains.\nHere we mainly focus on a simplified poker game that can be solved in under a minute, but the same core principles apply for scaled up versions that are much larger and more complex, though they will also involve approximating states of the game.\nThe richness of the game is an ideal testbed for decision making under uncertainty, probabilistic reasoning, and taking actions that can have both immediate and longer horizon consequences.\nWe can use the bot solutions to gain insights from the underlying game itself. For example, you’ll see that bots automatically learn to bluff, showing that this is a theoretically correct play and not a “loss leader” just to get more action later.\nBeyond building a basic bot, there is a broad area for further research, including agent evaluation, opponent modeling, building an LLM model on top of the traditional poker agent, and intepretability of the agent."
  },
  {
    "objectID": "about.html#modern-ai-agents",
    "href": "about.html#modern-ai-agents",
    "title": "C2C: About",
    "section": "Modern AI agents",
    "text": "Modern AI agents\nLLM-based AI agents are very popular in 2024. The poker bot we’re building here is different and uses the classic AI method of reinforcement learning, where the goal is to maximize long-term reward given feedback from the environment (i.e. maximizing winnings in a poker game).\nThe poker bot learns how to make optimal decisions given a state of the game by repeatedly playing the game against itself and optimizing the strategy over time.\nLLM agents generally use large amounts of text data to understand and generate human-like language based on patterns in the data, which could include strategy in a game like poker.\nSo is RL still relevant? Yes! OpenAI recently released their o1 chain of thought model:\n\nOur large-scale reinforcement learning algorithm teaches the model how to think productively using its chain of thought in a highly data-efficient training process.\n\nIn chain of thought, each step in the reasoning process can be seen as a state with possible actions. RLHF (Reinforcement Learning from Human Feedback) is also a popular technique for LLM agents where a human acts as the reward signal to shape a model’s outputs.\nHybrid approaches seem promising and may be a future direction for poker agents as well."
  },
  {
    "objectID": "about.html#inspiration",
    "href": "about.html#inspiration",
    "title": "C2C: About",
    "section": "Inspiration",
    "text": "Inspiration\nSome inspiration for this course:\n\nNand to Tetris: Building a Modern Computer From First Principles\nNeural Networks: Zero to Hero: A course by Andrej Karpathy on building neural networks, from scratch, in code\nfast.ai’s Practical Deep Learning: A free course designed for people with some coding experience, who want to learn how to apply deep learning and machine learning to practical problems"
  },
  {
    "objectID": "3_gametrees.html",
    "href": "3_gametrees.html",
    "title": "C2C #3: Game Trees",
    "section": "",
    "text": "The standard way to solve a game tree like this is using backward induction, whereby we start with the leaves (i.e. the payoff nodes at the bottom) of the tree and see which decisions the last player, Player 2 in this case, will make at her decision nodes.\nPlayer 2’s goal is to minimize the maximum payoff of Player 1, which in the zero-sum setting is equivalent to minimizing her own maximum loss or maximizing her own minimum payoff. This is equivalent to a Nash equilibrium in the zero-sum setting.\nShe picks the right node on the left side (payoff -1 instead of -5) and the left node on the right side (payoff 3 instead of -6).\nThese values are then propagated up the tree so from Player 1’s perspective, the value of going left is 1 and of going right is -3. The other leaf nodes are not considered because Player 2 will never choose those. Player 1 then decides to play left to maximize his payoff.\n\nWe can see all possible payouts, where the rows are P1 actions and the columns are P2 actions after P1 actions (e.g. Left/Left means P1 chose Left and then P2 also chose Left).\n\n\n\nP1/P2\nLeft/Left\nLeft/Right\nRight/Left\nRight/Right\n\n\n\n\nLeft\n5,-5\n5,-5\n1,-1\n1,-1\n\n\nRight\n-3,3\n-3,3\n6,-6\n6,-6\n\n\n\nNote that Player 1 choosing right could result in a higher payout (6) if Player 2 also chose right, but a rational Player 2 would not do that, and so the algorithm requires maximizing one’s minimum payoff, which means Player 1 must choose left (earning a guaranteed value of 1).\nBy working backwards from the end of a game, we can evaluate each possible sequence of moves and propagate the values up the game tree.",
    "crumbs": [
      "About",
      "#3: Game Trees"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Cards to Code",
    "section": "",
    "text": "A course by Max Chiswick and Ross Rheingans-Yoo on building a basic poker bot from scratch.\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe course starts with game theory, then introduces a simplified poker setting, then the elements of the Counterfactual Regret Minimization (CFR) algorithm, and then beyond the vanilla CFR algorithm.\nThroughout the course, we’re building up code to first develop the game, and then piece by piece to develop the poker bot.\nWe end on opponent modeling and challenge problems that go beyond game theory optimal play.\nPrerequisites: Basic Python and math (no poker knowledge necessary)\nGroups/Questions: We encourage you to work through this in groups. You can chat in our Discord to discuss the course or look for others to work with.\nGet started now! Start with the Setup and Face-up Poker page.",
    "crumbs": [
      "About",
      "Home"
    ]
  },
  {
    "objectID": "1_gametheory.html",
    "href": "1_gametheory.html",
    "title": "C2C #1: Game Theory",
    "section": "",
    "text": "The most simple type of game is the one that we built in the setup section. Cards get flipped over and you either win or lose. Not much skill…and not much fun!\nOther games like Bingo, Dreidel, and of course Snakes and Ladders can deceptively seem like you have strategic control of the game, but really this is just the “illusion of agency”, since you’re only, for example, rolling dice to randomize your next move. A disturbing fact:\n\nThere are three main classes of games that do have significant strategic agency: perfect information, perfect information with randomness, and imperfect (hidden) information. Perfect information means all game information is visible, while imperfect information meanas there is hidden/private information.\nWe can also think about games along the axis of fixed or random opponents and adversarial opponents.\n(There are also other game classes like sequential vs. simultaneous action games.)\n\n\n\n\n\n\n\n\nGame/Opponent\nFixed/Random Opponent\nAdversarial Opponent\n\n\n\n\nNo Player Agency/Decisions\nCandy Land, War, Dreidel, Bingo, Snakes and Ladders\nBlackjack dealer\n\n\nPerfect Info\nPuzzles, Rubik’s Cube\nTictactoe, Checkers, Chess, Arimaa, Go, Connect Four\n\n\nPerfect Info with Randomness\nMonopoly, Risk\nBackgammon\n\n\nImperfect Info\nWordle, Blackjack\nPoker, Rock Paper Scissors, Figgie, StarCraft\n\n\n\n\n\nPerfect information games like Chess and Go are complex and have been the focus of recent AI research (along with poker). Solving these games can theoretically be done using backward induction, which means starting from possible end positions and working backwards.\n\n\n\nHere we want to primarily focus on the bottom right of this chart: imperfect information games with an adversarial opponent.\nThese games cannot be solved in the same way as perfect information games because we don’t know always what true state of the game we are in because of the hidden information (we’ll look at this more in Game Trees).\nImperfect information games are representative of real world situations where information is usually incomplete, e.g. job interviews, investments, dating, and negotiations.\n\n\n\nThe goal of a game is to maximize the utility, which is usually a score or money or some kind of value.\nThere are two fundamental strategies:\n\nExploiting opponent weaknesses: Capitalize on specific flaws or tendencies of opponents, while putting yourself at risk if the opponent adapts well.\nBeing unexploitable: Playing a balanced, theoretically sound strategy, also known as game theory optimal. This does not maximize against weaker opponents, but rather aims to minimize losses against any possible opponent.",
    "crumbs": [
      "About",
      "#1: Game Theory"
    ]
  },
  {
    "objectID": "1_gametheory.html#types-of-games",
    "href": "1_gametheory.html#types-of-games",
    "title": "C2C #1: Game Theory",
    "section": "",
    "text": "The most simple type of game is the one that we built in the setup section. Cards get flipped over and you either win or lose. Not much skill…and not much fun!\nOther games like Bingo, Dreidel, and of course Snakes and Ladders can deceptively seem like you have strategic control of the game, but really this is just the “illusion of agency”, since you’re only, for example, rolling dice to randomize your next move. A disturbing fact:\n\nThere are three main classes of games that do have significant strategic agency: perfect information, perfect information with randomness, and imperfect (hidden) information. Perfect information means all game information is visible, while imperfect information meanas there is hidden/private information.\nWe can also think about games along the axis of fixed or random opponents and adversarial opponents.\n(There are also other game classes like sequential vs. simultaneous action games.)\n\n\n\n\n\n\n\n\nGame/Opponent\nFixed/Random Opponent\nAdversarial Opponent\n\n\n\n\nNo Player Agency/Decisions\nCandy Land, War, Dreidel, Bingo, Snakes and Ladders\nBlackjack dealer\n\n\nPerfect Info\nPuzzles, Rubik’s Cube\nTictactoe, Checkers, Chess, Arimaa, Go, Connect Four\n\n\nPerfect Info with Randomness\nMonopoly, Risk\nBackgammon\n\n\nImperfect Info\nWordle, Blackjack\nPoker, Rock Paper Scissors, Figgie, StarCraft\n\n\n\n\n\nPerfect information games like Chess and Go are complex and have been the focus of recent AI research (along with poker). Solving these games can theoretically be done using backward induction, which means starting from possible end positions and working backwards.\n\n\n\nHere we want to primarily focus on the bottom right of this chart: imperfect information games with an adversarial opponent.\nThese games cannot be solved in the same way as perfect information games because we don’t know always what true state of the game we are in because of the hidden information (we’ll look at this more in Game Trees).\nImperfect information games are representative of real world situations where information is usually incomplete, e.g. job interviews, investments, dating, and negotiations.\n\n\n\nThe goal of a game is to maximize the utility, which is usually a score or money or some kind of value.\nThere are two fundamental strategies:\n\nExploiting opponent weaknesses: Capitalize on specific flaws or tendencies of opponents, while putting yourself at risk if the opponent adapts well.\nBeing unexploitable: Playing a balanced, theoretically sound strategy, also known as game theory optimal. This does not maximize against weaker opponents, but rather aims to minimize losses against any possible opponent.",
    "crumbs": [
      "About",
      "#1: Game Theory"
    ]
  },
  {
    "objectID": "1_gametheory.html#rock-paper-scissors",
    "href": "1_gametheory.html#rock-paper-scissors",
    "title": "C2C #1: Game Theory",
    "section": "Rock Paper Scissors",
    "text": "Rock Paper Scissors\n\n\n\nImage by Enzoklop under CC BY-SA 3.0\n\n\n\nRock defeats scissors, scissors defeats paper, and paper defeats rock\nYou get +1 point for a win, -1 for a loss, and 0 for ties\n\nThis simple game was the subject of a DeepMind paper in 2023, where they wrote:\n\nIn sequential decision-making, agent evaluation has largely been restricted to few interactions against experts, with the aim to reach some desired level of performance (e.g. beating a human professional player). We propose a benchmark for multiagent learning based on repeated play of the simple game Rock, Paper, Scissors.\n\nLet’s dig in to why RPS is a more interesting game than it might seem.\n\nPayoff matrix\nThe core features of a game are its players, the actions of each player, and the payoffs. We can show this for RPS in the below payoff matrix, also known as normal-form.\n\n\n\nPlayer 1/2\nRock\nPaper\nScissors\n\n\n\n\nRock\n(0, 0)\n(-1, 1)\n(1, -1)\n\n\nPaper\n(1, -1)\n(0, 0)\n(-1, 1)\n\n\nScissors\n(-1, 1)\n(1, -1)\n(0, 0)\n\n\n\nThe payoffs for Player 1 are on the left and for Player 2 on the right in each payoff outcome of the game. For example, the bottom left payoff is when Player 1 plays Scissors and Player 2 plays Rock, resulting in -1 for P1 and +1 for P2.\nA strategy says which actions you would take for every state of the game.\n\n\nExpected value\nExpected value in a game represents the average outcome of a strategy if it were repeated many times. It’s calculated by multiplying each possible outcome by its probability of occurrence and then summing these products.\nSuppose that Player 1 plays the strategy:\n\n\\begin{cases}\nr = 0.5 \\\\\np = 0.25 \\\\\ns = 0.25\n\\end{cases}\n\nand Player 2 plays the strategy:\n\n\\begin{cases}\nr = 0.1 \\\\\np = 0.3 \\\\\ns = 0.6\n\\end{cases}\n\nLet’s add these to the matrix:\n\n\n\n\n\n\n\n\n\nPlayer 1/2\nRock (r_2=0.1)\nPaper (p_2=0.3)\nScissors (s_2=0.6)\n\n\n\n\nRock (r_1=0.5)\n(0, 0)\n(-1, 1)\n(1, -1)\n\n\nPaper (p_1=0.25)\n(1, -1)\n(0, 0)\n(-1, 1)\n\n\nScissors (s_1=0.25)\n(-1, 1)\n(1, -1)\n(0, 0)\n\n\n\nTo simplify, let’s just write the payoffs for Player 1 since payoffs for Player 2 will simply be the opposite:\n\n\n\n\n\n\n\n\n\nPlayer 1/2\nRock (r_2=0.1)\nPaper (p_2=0.3)\nScissors (s_2=0.6)\n\n\n\n\nRock (r_1=0.5)\n0\n-1\n1\n\n\nPaper (p_1=0.25)\n1\n0\n-1\n\n\nScissors (s_1=0.25)\n-1\n1\n0\n\n\n\nNow we can multiply the player action strategies together to get a percentage occurrence for each payoff in the matrix:\n\n\n\n\n\n\n\n\n\nPlayer 1/2\nRock (r_2=0.1)\nPaper (p_2=0.3)\nScissors (s_2=0.6)\n\n\n\n\nRock (r_1=0.5)\nVal: 0 Pr: 0.5(0.1) = 0.05\nVal: -1 Pr: 0.5(0.3) = 0.15\nVal: 1 Pr: 0.5(0.6) = 0.3\n\n\nPaper (p_1=0.25)\nVal: 1 Pr: 0.25(0.1) = 0.025\nVal: 0 Pr: 0.25(0.3) = 0.075\nVal: -1 Pr: 0.25(0.6) = .15\n\n\nScissors (s_1=0.25)\nVal: -1 Pr: 0.25(0.1) = 0.025\nVal: 1 Pr: 0.25(0.3) = 0.075\nVal: 0 Pr: 0.25(0.6) = .15\n\n\n\nNote that the total probabilities sum to 1 and each row and column sums to the probability of playing that row or column.\nWe can work out the expected value of the game to Player 1 (summing all payoffs multiplied by probabilities from top left to bottom right):\n\\mathbb{E}[P_1] = 0(0.05) + -1(0.15) + 1(0.3) + 1(0.025) + 0(0.075) + -1(0.15) + -1(0.025) + 1(0.075) + 0(0.15) = 0.075\nTherefore P1 is expected to gain 0.075 per game given these strategies. Since payoffs are reversed for P2, P2’s expectation is -0.075 per game.\n\n\nZero-sum\nWe see in the matrix that every payoff is zero-sum, i.e. the sum of the payoffs to both players is 0. This means the game is one of pure competition. Any amount P1 wins is from P2 and vice versa.\n\n\nNash equilibrium\nA Nash equilibrium means that no player can improve their expected payoff by unilaterally changing their strategy. That is, changing one’s strategy can only result in the same or worse payoff (assuming the other player does not change).\nIn RPS, the Nash equilibrium strategy is to play each action r = p = s = 1/3 of the time. I.e., to play totally randomly.\nPlayed a combination of strategies is called a mixed strategy, as opposed to a pure strategy, which would select only one action. Mixed strategies are useful in games of imperfect information because it’s valuable to not be predictable and to conceal your private information. In perfect information games, the theoretically optimal play would not contain any mixing (i.e., if you could calculate all possible moves to the end of the game).\nThe equilibrium RPS strategy is worked out below:\n\n\n\n\n\n\nNash equilibrium strategy for RPS\n\n\n\n\n\n\n\n\nPlayer 1/2\nRock\nPaper\nScissors\n\n\n\n\nRock (r)\n(0, 0)\n(-1, 1)\n(1, -1)\n\n\nPaper (p)\n(1, -1)\n(0, 0)\n(-1, 1)\n\n\nScissors (s)\n(-1, 1)\n(1, -1)\n(0, 0)\n\n\n\nIf Player 1 plays Rock with probability r, Paper with probability p, and Scissors with probability s, we have the following expected value equations for Player 2:\n\\mathbb{E}[P_2(R)] = -1p + 1s\n\\mathbb{E}[P_2(P)] = 1r - 1s\n\\mathbb{E}[P_2(S)] = -1r + 1p\n(To compute each of these, sum the payoffs for each column with P2 payoffs and P1 probabilities. P2 payoffs because these are the expected values for P2 and P1 probabiltiies because the payoffs depend on the strategy probabilties of P1 against each of P2’s actions.)\nSince no action dominates, we know that the EV of every strategic action should be equal (since if a certain strategy was best, we’d want to always play that strategy).\nTo solve for r, p, and s, we can start by setting these EVs equal:\n\\mathbb{E}[P_2(R)] = \\mathbb[P_2(P)]\n-1p + 1s = 1r - 1s\n2s = p + r\nThen setting these equal:\n\\mathbb{E}[P_2(R)] = \\mathbb{E}[P_2(S)]\n-1p + 1s = -1r + 1p\ns + r = 2p\nAnd finally setting these equal:\n\\mathbb{E}[P_2(P)] = \\mathbb{E}[P_2(S)]\n1r - 1s = -1r + 1p\n2r = s + p\nNow we have these equations:\n\n\\begin{cases}\n2s = p + r \\\\\ns + r = 2p \\\\\n2r = s + p\n\\end{cases}\n\nWe can rewrite the 1st:\nr = 2s - p\nAnd combine with the 2nd:\ns + (2s - p) = 2p\n3s = 3p\nResulting in:\ns = p\nNow we can go back to the 2nd equation:\ns + r = 2p\nAnd insert s = p:\ns + r = 2s\nAnd arrive at:\nr = s\nWe now see that all are equal:\ns = p = r\nWe also know that they must all sum to 1:\nr + p + s = 1\nSince they’re all equal and sum to 1, we can substitute p and s with r:\n3r = 1\nr = 1/3\nSo all actions are taken with probability 1/3:\nr = p = s = 1/3 \\quad \\blacksquare\nBy symmetry, the same equilibrium strategy is true for Player 2.\n\n\n\nPlaying this strategy means that whatever your opponent does, you will breakeven! For example, think about an opponent that always plays Rock.\n\n\\begin{equation}\n\\begin{split}\n\\mathbb{E}(\\text{Equilibrium vs. Rock}) &= 0(r) + 1(p) + -1(s) \\\\\n&= 0(1/3) + 1(1/3) + -1(1/3) \\\\\n&= 0\n\\end{split}\n\\end{equation}\n\nHow about the case of the opponent playing 60% Rock, 20% Paper, 20% Scissors?\n\n\\begin{equation}\n\\begin{split}\n\\mathbb{E}(\\text{Equilibrium vs. 622}) &= 0.6(\\text{Equilibrium vs. Rock}) \\\\\n&\\quad{}+ 0.2(\\text{Equilibrium vs. Paper}) \\\\  \n&\\quad{}+ 0.2(\\text{Equilibrium vs. Scissors}) \\\\\n&= 0.6(0) + 0.2(0) + 0.2(0) \\\\\n&= 0\n\\end{split}\n\\end{equation}\n\nThe random equilibrium strategy will result in 0 against any pure strategy and any combination of strategies including 622 and the opponent playing the random strategy.\n\n\nExploiting vs. Nash\nThe equilibrium strategy vs. a pure Rock opponent is a useful illustration of the limitations of playing at equilibrium. The Rock opponent is playing the worst possible strategy, yet equilibrium is still breaking even!\nWhat’s the best that we could do against Rock only? We could play purely paper. The payoffs are written for playing Paper and the probabilities indicate the opponent playing only Rock.\n\n\\begin{equation}\n\\begin{split}\n\\mathbb{E}(\\text{Paper vs. Rock}) &= 1(r) + 0(p) + -1(s) \\\\\n&= 1(1) + 0(0) + -1(0) \\\\\n&= 1\n\\end{split}\n\\end{equation}\n\nWe’d win 1 each game playing Paper vs. Rock.\nHow about against the opponent playing 60% Rock, 20% Paper, 20% Scissors? Here we can see that because they are overplaying Rock, our best strategy is again to always play Paper. We write the payoffs for playing Paper and the probabilities according to the 622 strategy.\n\n\\begin{equation}\n\\begin{split}\n\\mathbb{E}(\\text{Paper vs. 622}) &= 1(r) + 0(p) + -1(s) \\\\\n&= 1(0.6) + 0(0.2) + -1(0.2) \\\\\n&= 0.6 + 0 - 0.2 \\\\\n&= 0.4\n\\end{split}\n\\end{equation}\n\nPlaying Paper vs. 622 results in an expected win of 0.4 per game.",
    "crumbs": [
      "About",
      "#1: Game Theory"
    ]
  },
  {
    "objectID": "1_gametheory.html#soccer-kicker-vs.-goalie",
    "href": "1_gametheory.html#soccer-kicker-vs.-goalie",
    "title": "C2C #1: Game Theory",
    "section": "Soccer Kicker vs. Goalie",
    "text": "Soccer Kicker vs. Goalie\nConsider the Soccer Penalty Kick game where a Kicker is trying to score a goal and the Goalie is trying to block it.\n\n\n\nKicker/Goalie\nLean Left\nLean Right\n\n\n\n\nKick Left\n(0, 0)\n(2, -2)\n\n\nKick Right\n(1, -1)\n(0, 0)\n\n\n\nThe game setup is zero-sum. If Kicker and Goalie both go in one direction, then it’s assumed that the goal will miss and both get 0 payoffs. If the Kicker plays Kick Right when the Goalie plays Lean Left, then the Kicker is favored and gets a payoff of +1. If the Kicker plays Kick Left when the Goalie plays Lean Right, then the kicker is even more favored, because it’s easier to kick left than right, and gets +2.\n\nNash equilibrium?\nWhich of these, if any, is a Nash equilibrium? You can check by seeing if either player would benefit by changing their action.\n\n\n\nKicker\nGoalie\nEquilibrium or Change?\n\n\n\n\nLeft\nLeft\n\n\n\nLeft\nRight\n\n\n\nRight\nLeft\n\n\n\nRight\nRight\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThere are no pure Nash equilibrium solutions because when the actions match, the Kicker will always want to change, and when they don’t match, the Goalie will always want to change.\n\n\n\nKicker\nGoalie\nEquilibrium or Change?\n\n\n\n\nLeft\nLeft\nKicker changes to right\n\n\nLeft\nRight\nGoalie changes to left\n\n\nRight\nLeft\nGoalie changes to right\n\n\nRight\nRight\nKicker changes to left\n\n\n\n\n\n\nAssume that they both play randomly – Left 50% and Right 50% – what is the expected value of the game?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nKicker/Goalie\nLean Left (0.5)\nLean Right (0.5)\n\n\n\n\nKick Left (0.5)\n(0, 0)\n(2, -2)\n\n\nKick Right (0.5)\n(1, -1)\n(0, 0)\n\n\n\nWe apply these probabilities to each of the 4 outcomes and write in terms of the Kicker payoffs:\n\n\n\nKicker/Goalie\nLean Left (0.5)\nLean Right (0.5)\n\n\n\n\nKick Left (0.5)\nVal: 0 Pr: 0.25\nVal: 2 Pr: 0.25\n\n\nKick Right (0.5)\nVal: 1 Pr: 0.25\nVal: 0 Pr: 0.25\n\n\n\nNow for the Kicker, we have \\mathbb{E}[K] = 0.25(0) + 0.25(2) + 0.25(1) + 0.25(0) = 0.75.\nSince it’s zero-sum, we have \\mathbb{E}[G] = -0.75 for the Goalie.\n\n\n\n\n\nProbabilistic interpretations\nNote that, for example, the Kicker playing 50% Left and 50% Right could be interpreted as a single player having these probabilities or a field of players averaging to these probabilities. So out of 100 players, this could mean:\n\n100 players playing 50% Left and 50% Right\n50 players playing 100% Left and 50 players playing 100% Right\n50 players playing 75% Left/25% Right and 50 players playing 25% Left/75% right\n\n\n\nFinding equilibrium\nWhen the Goalie plays left with probability p and right with probability 1-p, we can find the expected value of the Kicker actions.\n\n\n\nKicker/Goalie\nLean Left (p)\nLean Right (1-p)\n\n\n\n\nKick Left\n(0, 0)\n(2, -2)\n\n\nKick Right\n(1, -1)\n(0, 0)\n\n\n\n\\mathbb{E}[K(L)] = p(0) + (1-p)(2) = 2 - 2p\n\\mathbb{E}[K(R)] = p(1) + (1-p)(0) = p\nThe Kicker is going to play the best response to the Goalie’s strategy. The Goalie wants to make the Kicker indifferent to Kick Left and Kick Right because if the Kicker was not going to be indifferent, then he would prefer one of the actions, meaning that action would be superior to the other. When you make your opponent indifferent, then you don’t give them any best play.\nTherefore the Kicker will play a mixed strategy in response that will result in a Nash equilibrium where neither player benefits from unilaterally changing strategies. (Note that indifferent does not mean 50% each, but means the expected value is the same for each.)\n\nBy setting the values equal, we get 2 - 2p = p \\Rightarrow p = \\frac{2}{3} as shown in the plot.\nThis means that 1-p = 1 - \\frac{2}{3} = \\frac{1}{3}.\nTherefore the Goalie should play Lean Left (p) \\frac{2}{3} and Lean Right (1-p) \\frac{1}{3}.\nThe value for the Kicker is \\frac{2}{3}, or (0.67), for both actions, regardless of the Kicker’s mixing strategy.\nNote that the Kicker is worse off now (0.67 now compared to 0.75) than when both players played 50% each action. Why?\nIf the Kicker plays Kick Left with probability q and Kick Right with probability 1-q, then the Goalie’s values are:\n\\mathbb{E}[G(L)] = 0(q) - 1(1-q) = -1 + q\n\\mathbb{E}[G(R)] = -2q + 0(1-q) = -2q\nSetting equal,\n\n\\begin{equation}\n\\begin{split}\n-1 + q &= -2q \\\\\n-1 &= -3q  \\\\\n\\frac{1}{3} &= q\n\\end{split}\n\\end{equation}\n\nTherefore the Kicker should play Kick Left (q) \\frac{1}{3} and Kick Right (1-q) \\frac{2}{3}, giving a value of -\\frac{2}{3} to the Goalie.\nWe can see this from the game table:\n\n\n\n\n\n\n\n\nKicker/Goalie\nLean Left (\\frac{2}{3})\nLean Right (\\frac{1}{3})\n\n\n\n\nKick Left (\\frac{1}{3})\nVal: (0, 0) Pr: \\frac{2}{9}\nVal: (2, -2) Pr: \\frac{1}{9}\n\n\nKick Right (\\frac{2}{3})\nVal: (1, -1) Pr: \\frac{4}{9}\nVal: (0, 0) Pr: \\frac{2}{9}\n\n\n\nTherefore the expected payoffs in this game are:\n\\mathbb{E}[K] = \\frac{2}{9}(0) + \\frac{1}{9}(2) + \\frac{4}{9}(1) + \\frac{2}{9}(0) = \\frac{6}{9} = 0.67 for the Kicker and -0.67 for the Goalie.\n\n\nSwitching out of equilibrium\nIn equilibrium, both players are playing a best response against each other, giving no incentive to deviate.\nTherefore no player can unilaterally improve by changing their strategy. What if the Kicker switches to always Kick Left?\n\n\n\n\n\n\n\n\nKicker/Goalie\nLean Left (\\frac{2}{3})\nLean Right (\\frac{1}{3})\n\n\n\n\nKick Left (1)\nVal: (0, 0) Pr: \\frac{2}{3}\nVal: (2, -2) Pr: \\frac{1}{3}\n\n\nKick Right (0)\nVal: (1, -1) Pr: 0\nVal: (0, 0) Pr: 0\n\n\n\nNow the Kicker’s payoff is still \\frac{2}{3}(0) + \\frac{1}{3}(2) = 0.67.\nWhen a player makes their opponent indifferent, this means that any action the opponent takes (within the set of equilibrium actions) will result in the same payoff! But now they are no longer in equilibrium.\nSo if you know your opponent is playing the equilibrium strategy, then you can actually do whatever you want with no penalty with the mixing actions. Sort of.\nThe risk is that the opponent can now deviate from equilibrium and take advantage of your new strategy. For example, if the Goalie caught on and moved to always Lean Left, then the value is reduced to 0 for both players. Though if the Kicker then caught on and moved to always Kick Right, then the value goes up to 1 for the Kicker and -1 for the Goalie. Perhaps after a few cycles like this, the players would go back to equilibrium.\nTo summarize, you can only be penalized for not playing the equilibrium mixing strategy if your opponent plays a non-equilibrium strategy that exploits your strategy. You can only profit more than the equilibrium strategy if your opponent either plays off equilibrium and you do as well or if you play equilibrium and they play some action that is not part of the equilibrium strategy (also known as the support, or the set of pure strategies that are played with non-zero probability under the mixed strategy, which is not possible in this game since both possible actions are played at equilibrium).",
    "crumbs": [
      "About",
      "#1: Game Theory"
    ]
  },
  {
    "objectID": "1_gametheory.html#clairvoyance-poker",
    "href": "1_gametheory.html#clairvoyance-poker",
    "title": "C2C #1: Game Theory",
    "section": "Clairvoyance Poker",
    "text": "Clairvoyance Poker\nBack to poker. We previously built Face-up 3-card Poker where each player was dealt a card from the deck of {0, 1, 2} and the high card won.\nNow we’re going to make a small change to the game, while keeping the same deck.\n\nPlayer 1 will always be dealt card 1.\nPlayer 2 will be dealt randomly from {0, 2}.\nP1 knows P2 has either card 0 or 2, but not which. P2 knows that P1 has card 1.\nThis is representative of a real poker scenario where P1 has a mid-strength hand that can beat bluffs, but loses to value bets by strong hands. P2 has either a bluff (weak hand) or a strong hand.\n\nLet’s modify the code to update the deal:\n\n\n\n\n\n\nUpdate deal_cards function\n\n\n\n\n\n    def deal_cards(self):\n        self.cards[0] = 1\n        remaining_cards = [card for card in self.deck if card != self.cards[0]]\n        self.cards[1] = random.choice(remaining_cards)\n\n\n\n\nEquity and Variance\nEquity is defined as the percentage of the pot expected to win given the cards, assuming no additional betting. We see that in the original case where players were randomly dealt cards and in this case, both players have 50% equity before the cards are dealt. After the cards are dealt, the higher card has 100% equity.\nIn real poker games like Texas Hold’em, equity is more interesting because there are additional community cards that mean that even if one hand starts off as stronger, it won’t necessarily win. Below we have a special poker hand where the equities are shown as 53% and 47% and the hand currently losing actually has higher equity!\n\nVariance measures the spread of the score differences. Higher variance means more volatility in the outcomes.\nWe can run a sample of games and get an empirical sample for:\n\nEquity:\nVariance\nWinrate:\n\nStandard Error: This measures the precision of our sample mean as an estimate of the population mean. Confidence Interval: This gives us a range where we expect the true population mean to lie with 95% confidence.\n\n\nBetting Rules and Sequences\nLet’s make this a real strategic poker game with betting. This game, Clairvoyance Poker, was first shown in The Mathematics of Poker (Bill Chen, Jared Ankenman) and then also in the excellent Play Optimal Poker (Andrew Brokos).\n\nBoth players ante 1 chip, so the starting pot is 2\nEach player has 1 remaining chip to use for betting\nPlayer 1 acts first and can either Bet 1 or Check (Pass)\n\nPlay continues until one of these betting sequences happens, which ends the hand:\n\nCheck Check: Hands go to showdown and player with highest card wins 1 (the ante)\nBet Call: Hands go to showdown and player with highest card wins 2 (the ante + bet)\nBet Fold: Player who bets wins 1 (the ante)\n\nHere is a list of all possible betting sequences:\n\n\n\nP1\nP2\nP1\nWinner\n\n\n\n\nBet\nFold\n\nP1 (+1)\n\n\nBet\nCall\n\nHigher Card (+2)\n\n\nCheck\nCheck\n\nHigher Card (+1)\n\n\nCheck\nBet\nFold\nP2 (+1)\n\n\nCheck\nBet\nCall\nHigher Card (+2)\n\n\n\n\n\nStarting Strategies\nThere are six possible states of the game, two for P1 and four for P2. Let’s hard-code some initial strategies for both players:\n\nP1[1, Starting]: Randomly Bet/Check\nP2[0, After Bet]: Fold\nP2[2, After Bet]: Call\nP2[0, After Check]: Randomly Bet/Check\nP2[2, After Check]: Bet\nP1[1, After Check and Bet]: Randomly Bet/Check\n\nThis strategy randomizes uncertain decisions and takes clearly smart decisions when possible. What does that mean?\n\nP2[0, After Bet]: Fold\n\nThis is the worst card and can’t win\n\nP2[2, After Bet]: Call\n\nThis is the best hand and guarantees a win\n\nP2[2, After Check]: Bet\n\nThis is the best card and betting can only possibly win additional chips\n\n\nLet’s put the strategies into our code and see how each strategy is doing. There are 9 total state-action pairs bolded below:\n\nP1[1, Starting]: Randomly Bet/Check\n\nP1[1, Starting]: Bet\nP1[1, Starting]: Check\n\nP2[0, After Bet]: Fold\nP2[2, After Bet]: Call\nP2[0, After Check]: Randomly Bet/Check\n\nP2[0, After Check]: Bet\nP2[0, After Check]: Check\n\nP2[2, After Check]: Bet\nP1[1, After Check and Bet]: Randomly Call/Fold\n\nP1[1, After Check and Bet]: Call\nP1[1, After Check and Bet]: Fold\n\n\nFor now, we’ll do this in a simple, but not-so-elegant way.\n—&gt; Logic for taking these actions\n—&gt; Logic for keeping track of frequency of each situation/action and results for each situation/action (and still output stuff from before)\n\n\nDominated Strategy\nNote that P1 betting is strictly worse and why –&gt; show math\n–&gt; Update logic to always check\n\nP1[1, Starting]: Check\nP2[0, After Bet]: Fold\nP2[2, After Bet]: Call\nP2[0, After Check]: Randomly Bet/Check\n\nP2[0, After Check]: Bet\nP2[0, After Check]: Check\n\nP2[2, After Check]: Bet\nP1[1, After Check and Bet]: Randomly Call/Fold\n\nP1[1, After Check and Bet]: Call\nP1[1, After Check and Bet]: Fold\n\n\n\n\nYour Strategies\n—&gt; Try strat for P1 then for P2 See results\n\nP1[1, Starting]: Check\nP2[0, After Bet]: Fold\nP2[2, After Bet]: Call\nP2[0, After Check]: ??\nP2[2, After Check]: Bet\nP1[1, After Check and Bet]: ??\n\n\n\nIndifference in Poker\nThere are two strategies for which we can find an equilibrium:\n\nP2[0, After Check]: ??\nP1[1, After Check and Bet]: ??\n\nLet’s start by looking at the EVs for P2’s action with card 0 facing a Check.\n\\mathbb{E}[P2](0,Check) = 0 \\mathbb{E}[P2](0,Bet) = 2(\\Pr(\\text{P1 Fold})) + -1(\\Pr(\\text{P1 Call}))\nThese should be equal in order for P2 to be indifferent. Let \\Pr(\\text{P1 Call}) = p and \\Pr(\\text{P1 Fold}) = 1-p.\nNote that we set the Check action to have an EV of 0 because we are counting any chips already in the pot as already used.\nWe solve for the P1 strategy that makes P2 indifferent.\n\n\\begin{equation}\n\\begin{split}\n0 &= 2(1-p) - p \\\\\n0 &= 2 - 2p - p \\\\\n3p &= 2 \\\\\np &= 2/3\n\\end{split}\n\\end{equation}\n\nTherefore P1 should play $p =\nNow let’s look at the EVs for P1’s action with card 1 facing a Bet after Checking.\n\\mathbb{E}[P1](1,Fold) = 0 \\mathbb{E}[P2](1,Call) = 3(\\Pr(\\text{P2 Card 0 and Bets})) + -1(\\Pr(\\text{P2 Card 2 and Bets}))\nWe know that \\Pr\\text{P2 Card 2 and Bets} = \\frac{1}{2}(1) since P2 has the card half the time and bets always with the card.\nLet b represent how often P2 bluffs with card 0.\n\\Pr(\\text{P2 Card 0 and Bets}) = (1/2)(b)\nNow to equate the two equations above, we have:\n0 = 3(1/2)(b) - 1/2 b = 1/3\nTherefore P2 will bet 1/3 of the time with card 0 facing a Check, i.e. P2 will bluff 1/3 of the time with the Q.\nNote that the indifference equation also accounts for how often P2 is betting with card 2 and we are really thinking about an overall strategy for playing the range of cards {0, 2}.\nOne additional step is that we can compute the overall ratio of bluffs vs. value bets that P1 is facing.\nP2 has card 0 and 2 each \\frac{1}{2} of the time.\nP2 is betting card 0 \\frac{1}{3} of the time (bluffing).\nP2 is betting card 2 always (value betting).\nTherefore for every 3 times with card 0 you will bet once and for every 3 times with card 2 you will bet 3 times. Out of the 4 bets, 1 of them is a bluff.\n\\Pr(\\text{P2 Bluff after P1 Check}) = \\frac{1}{4}\nOverall we have this strategy for P2:\n\n2/6: Give up (check) with card 0\n1/6: Bluff with card 0\n3/6: Value bet with card 2\n\nIf P2 bet \\frac{2}{3} instead of \\frac{1}{3} with card 0 after P1 checks and P1 is playing an equilibrium strategy, how would P2’s expected value change?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIt wouldn’t! As long as P1 doesn’t modify their equilibrium strategy, then P2 can mix his strategy however he wants and have the same EV.\n\n\n\n\n\nEquity vs. Expected Value\ncan tell opponent strategy to bet 1/3 0s but can’t stop call more catches more bluffs, but pays off more value bets\nSometimes have to bluff with bad cards because EV higher than if you always folded with bad cards.\nAdd uncertainty the opponent has to think about in counter-strategy If always folded bad cards, opponent knows you keep playing with good #\n\n\nexploits\nbet all good, check all bad opponent can always fold Then switch to always betting Then switch to awlasy calling Then move to betting strong, checking weak\nbluffing combined with value betting is the profitable thing. players who fold too often lose to bluffs, call too often lose value to value bets. betting to bluff and call ratios adepend on pot size\n\n\nBonus: Bet Sizing\npot sizing affects strategy pot large etc. –&gt; Add pot size stuff pot odds\n1 - bet/(bet+pot) larger bet means call less, let them get away with bluffs the bigger they bet\nbluff to value ratio bet/(bet+pot) bluffing frequency goes up as bet size gets larger relative to pot",
    "crumbs": [
      "About",
      "#1: Game Theory"
    ]
  }
]