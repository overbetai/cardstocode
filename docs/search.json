[
  {
    "objectID": "setup.html",
    "href": "setup.html",
    "title": "C2C: Setup and Face-up Poker",
    "section": "",
    "text": "You can use a code editor of your choice. We like VSCode. You can also work with Google Colab in the browser. The instructions below are for coding locally on MacOS or Unix.\n\n\nMake a local directory for the project cardstocode. From the command line:\nmkdir cardstocode\ncd cardstocode\n\n\n\nCreate and activate a virtual environment:\npython3 -m venv venv\nsource venv/bin/activate\nYou should now see (venv) at the beginning of your command prompt, indicating that the virtual environment is active.\n\n\n\nCreate a Python file c2c.py. From the command line:\ntouch c2c.py\n\n\n\nTo deactivate:\ndeactivate\nTo reactivate:\nsource venv/bin/activate\n\n\n\nInstall numpy, which we will use later.\npip install numpy",
    "crumbs": [
      "About",
      "Setup and Face-up Poker"
    ]
  },
  {
    "objectID": "setup.html#set-up-your-environment",
    "href": "setup.html#set-up-your-environment",
    "title": "C2C: Setup and Face-up Poker",
    "section": "",
    "text": "You can use a code editor of your choice. We like VSCode. You can also work with Google Colab in the browser. The instructions below are for coding locally on MacOS or Unix.\n\n\nMake a local directory for the project cardstocode. From the command line:\nmkdir cardstocode\ncd cardstocode\n\n\n\nCreate and activate a virtual environment:\npython3 -m venv venv\nsource venv/bin/activate\nYou should now see (venv) at the beginning of your command prompt, indicating that the virtual environment is active.\n\n\n\nCreate a Python file c2c.py. From the command line:\ntouch c2c.py\n\n\n\nTo deactivate:\ndeactivate\nTo reactivate:\nsource venv/bin/activate\n\n\n\nInstall numpy, which we will use later.\npip install numpy",
    "crumbs": [
      "About",
      "Setup and Face-up Poker"
    ]
  },
  {
    "objectID": "setup.html#face-up-1-card-poker",
    "href": "setup.html#face-up-1-card-poker",
    "title": "C2C: Setup and Face-up Poker",
    "section": "Face-up 1-card poker",
    "text": "Face-up 1-card poker\nWe’ll start by setting up a poker game with a 3-card deck where the cards are given values {0, 1, 2}. Each player is dealt a card “face-up” and the higher card wins. This is like the game War, but even more simplified because there are no ties since the entire deck is only 3 cards.\n\nStep 1: Imports\nLet’s start by importing numpy (will be used later) and random.\n\n\n\n\n\n\nImports\n\n\n\n\n\nimport numpy as np\nimport random\n\n\n\n\n\nStep 2: Set up the basic structure\nDefine a Game class that is initialized with these class attributes:\n\n3-card deck array with cards 0, 1, and 2\nCard placeholders for each player in a cards array that are initialized as None\nScore placeholders for each player in scores array that are initialized as 0\n\n\n\n\n\n\n\nInitialize Game class\n\n\n\n\n\nclass Game:\n    def __init__(self):\n        self.deck = [0, 1, 2]\n        self.cards = [None, None]\n        self.scores = [0, 0]\n\n\n\n\n\nStep 3: Deal cards\nLet’s now add a deal_cards function into the Game class that samples two cards from the deck into self.cards. This is the first of two class methods.\n\n\n\n\n\n\ndeal_cards function\n\n\n\n\n\n    def deal_cards(self):\n        self.cards = random.sample(self.deck, 2)\n\n\n\n\n\nStep 4: Play round\nCreate a play_round function in the Game class that first deals the cards using self.deal_cards and then accumulates the scores for each player using self.scores. This is the second class method.\n\n\n\n\n\n\nplay_round function\n\n\n\n\n\n    def play_round(self):\n        self.deal_cards()\n        if self.cards[0] &gt; self.cards[1]:\n            self.scores[0] += 1\n            self.scores[1] -= 1\n        elif self.cards[0] &lt; self.cards[1]:\n            self.scores[0] -= 1\n            self.scores[1] += 1\n\n\n\n\n\nStep 5: Run the game\nCreate the main function that defines num_games as 100, creates an object of the Game class in game, and runs the game.\n\n\n\n\n\n\nmain function\n\n\n\n\n\ndef main():\n    num_games = 100\n    game = Game()\n\n    for _ in range(num_games):\n        game.play_round()\nThe game object now has all the attributes and methods defined in the Game class.\n\n\n\n\n\nStep 6: Print the scores\nPrint the scores for each player after the game ends, also in the main function.\n\n\n\n\n\n\nPrint scores\n\n\n\n\n\n\n    print(f\"After {num_games} games:\")\n    print(f\"Player 1 score: {game.scores[0]}\")\n    print(f\"Player 2 score: {game.scores[1]}\")\n\n\n\n\n\nStep 7: Create the main block\n\n\n\n\n\n\nMain block\n\n\n\n\n\nif __name__ == \"__main__\":\n    main()\n\n\n\n\n\nPutting it all together\n\n\n\n\n\n\nAll the code\n\n\n\n\n\nimport random\n\nclass Game:\n    def __init__(self):\n        self.deck = [0, 1, 2]\n        self.cards = [None, None]  # Cards for player 0 and 1\n        self.scores = [0, 0]  # Scores for player 0 and 1\n\n    def deal_cards(self):\n        self.cards = random.sample(self.deck, 2)\n\n    def play_round(self):\n        self.deal_cards()\n        if self.cards[0] &gt; self.cards[1]:\n            self.scores[0] += 1\n            self.scores[1] -= 1\n        elif self.cards[0] &lt; self.cards[1]:\n            self.scores[0] -= 1\n            self.scores[1] += 1\n\ndef main():\n    num_games = 100\n    game = Game()\n\n    for _ in range(num_games):\n        game.play_round()\n\n    print(f\"After {num_games} games:\")\n    print(f\"Player 1 score: {game.scores[0]}\")\n    print(f\"Player 2 score: {game.scores[1]}\")\n\nif __name__ == \"__main__\":\n    main()\n\n\n\n\n\nRun the code\npython3 c2c.py\nThe output should look something like:\n\n\n\n\n\n\nSample code output from 100 games\n\n\n\n\n\nAfter 100 games:\nPlayer 1 score: -12\nPlayer 2 score: 12",
    "crumbs": [
      "About",
      "Setup and Face-up Poker"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "C2C: About",
    "section": "",
    "text": "We’re excited about learning through games. We developed Cards to Code as a foundational course on building a functional poker bot after running an AI Poker Camp course in San Francisco in summer 2024.\nCheck out Overbet.ai (under development) and Poker Camp.\nWe’ll also have more advanced courses available soon."
  },
  {
    "objectID": "about.html#about-us",
    "href": "about.html#about-us",
    "title": "C2C: About",
    "section": "",
    "text": "We’re excited about learning through games. We developed Cards to Code as a foundational course on building a functional poker bot after running an AI Poker Camp course in San Francisco in summer 2024.\nCheck out Overbet.ai (under development) and Poker Camp.\nWe’ll also have more advanced courses available soon."
  },
  {
    "objectID": "about.html#why-poker",
    "href": "about.html#why-poker",
    "title": "C2C: About",
    "section": "Why poker?",
    "text": "Why poker?\nLet’s split this into two questions.\n\nWhy play poker?\nGames of incomplete information like poker are most similar to real-life settings. Incomplete information means that there is some hidden information – in Texas Hold’em this is the opponent cards and the yet to be revealed board cards.\nThis means we need to infer our opponents’ hands and understand the probabilities of the cards that we can’t see.\nPoker is useful from a math perspective – probability, expected value, risk, and bankroll management are all important skills.\nIt’s also useful from a psychological perspective – emotional control, reading opponents, and adapting to other players are all valuable.\nWe learn to make decisions and focus on the quality of the decision, rather than the results.\n\n\nWhy build a poker bot?\nPoker has clear rules and structure, so we can apply reinforcement learning where the rewards are the profits in the game. Reinforcement learning, game theory, and Monte Carlo methods are applicable across many domains.\nHere we mainly focus on a simplified poker game that can be solved in under a minute, but the same core principles apply for scaled up versions that are much larger and more complex, though they will also involve approximating states of the game.\nThe richness of the game is an ideal testbed for decision making under uncertainty, probabilistic reasoning, and taking actions that can have both immediate and longer horizon consequences.\nWe can use the bot solutions to gain insights from the underlying game itself. For example, you’ll see that bots automatically learn to bluff, showing that this is a theoretically correct play and not a “loss leader” just to get more action later.\nBeyond building a basic bot, there is a broad area for further research, including agent evaluation, opponent modeling, building an LLM model on top of the traditional poker agent, and intepretability of the agent."
  },
  {
    "objectID": "about.html#modern-ai-agents",
    "href": "about.html#modern-ai-agents",
    "title": "C2C: About",
    "section": "Modern AI agents",
    "text": "Modern AI agents\nLLM-based AI agents are very popular in 2024. The poker bot we’re building here is different and uses the classic AI method of reinforcement learning, where the goal is to maximize long-term reward given feedback from the environment (i.e. maximizing winnings in a poker game).\nThe poker bot learns how to make optimal decisions given a state of the game by repeatedly playing the game against itself and optimizing the strategy over time.\nLLM agents generally use large amounts of text data to understand and generate human-like language based on patterns in the data, which could include strategy in a game like poker.\nSo is RL still relevant? Yes! OpenAI recently released their o1 chain of thought model:\n\nOur large-scale reinforcement learning algorithm teaches the model how to think productively using its chain of thought in a highly data-efficient training process.\n\nIn chain of thought, each step in the reasoning process can be seen as a state with possible actions. RLHF (Reinforcement Learning from Human Feedback) is also a popular technique for LLM agents where a human acts as the reward signal to shape a model’s outputs.\nHybrid approaches seem promising and may be a future direction for poker agents as well."
  },
  {
    "objectID": "about.html#inspiration",
    "href": "about.html#inspiration",
    "title": "C2C: About",
    "section": "Inspiration",
    "text": "Inspiration\nSome inspiration:\n\nNand to Tetris: Building a Modern Computer From First Principles\nNeural Networks: Zero to Hero: A course by Andrej Karpathy on building neural networks, from scratch, in code\nfast.ai’s Practical Deep Learning: A free course designed for people with some coding experience, who want to learn how to apply deep learning and machine learning to practical problems"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Cards to Code",
    "section": "",
    "text": "A course by Max Chiswick and Ross Rheingans-Yoo on building a basic poker bot from scratch.\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe course starts with game theory, then introduces a simplified poker setting, then the elements of the Counterfactual Regret Minimization (CFR) algorithm, and then beyond the vanilla CFR algorithm.\nThroughout the course, we’re building up code to first develop the game, and then piece by piece to develop the poker bot.\nWe end on opponent modeling and challenge problems that go beyond game theory optimal play.\nPrerequisites: Basic Python and math (no poker knowledge necessary)\nGroups/Questions: We encourage you to work through this in groups. You can chat in our Discord to discuss the course or look for others to work with.\nGet started now! Start with the Setup and Face-up Poker page.",
    "crumbs": [
      "About",
      "Home"
    ]
  }
]