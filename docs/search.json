[
  {
    "objectID": "setup.html",
    "href": "setup.html",
    "title": "C2C: Setup and Face-up Poker",
    "section": "",
    "text": "You can use a code editor of your choice. We like VSCode. You can also work with Google Colab in the browser. The instructions below are for coding locally on MacOS or Unix.\n\n\nMake a local directory for the project cardstocode. From the command line:\nmkdir cardstocode\ncd cardstocode\n\n\n\nCreate and activate a virtual environment:\npython3 -m venv venv\nsource venv/bin/activate\nYou should now see (venv) at the beginning of your command prompt, indicating that the virtual environment is active.\n\n\n\nCreate a Python file c2c.py. From the command line:\ntouch c2c.py\n\n\n\nTo deactivate:\ndeactivate\nTo reactivate:\nsource venv/bin/activate\n\n\n\nInstall numpy, which we will use later.\npip install numpy",
    "crumbs": [
      "About",
      "Setup and Face-up Poker"
    ]
  },
  {
    "objectID": "setup.html#set-up-your-environment",
    "href": "setup.html#set-up-your-environment",
    "title": "C2C: Setup and Face-up Poker",
    "section": "",
    "text": "You can use a code editor of your choice. We like VSCode. You can also work with Google Colab in the browser. The instructions below are for coding locally on MacOS or Unix.\n\n\nMake a local directory for the project cardstocode. From the command line:\nmkdir cardstocode\ncd cardstocode\n\n\n\nCreate and activate a virtual environment:\npython3 -m venv venv\nsource venv/bin/activate\nYou should now see (venv) at the beginning of your command prompt, indicating that the virtual environment is active.\n\n\n\nCreate a Python file c2c.py. From the command line:\ntouch c2c.py\n\n\n\nTo deactivate:\ndeactivate\nTo reactivate:\nsource venv/bin/activate\n\n\n\nInstall numpy, which we will use later.\npip install numpy",
    "crumbs": [
      "About",
      "Setup and Face-up Poker"
    ]
  },
  {
    "objectID": "setup.html#face-up-1-card-poker",
    "href": "setup.html#face-up-1-card-poker",
    "title": "C2C: Setup and Face-up Poker",
    "section": "Face-up 1-card poker",
    "text": "Face-up 1-card poker\nWe’ll start by setting up a poker game with a 3-card deck where the cards are given values {0, 1, 2}. Each player is dealt a card “face-up” and the higher card wins. This is like the game War, but even more simplified because there are no ties since the entire deck is only 3 cards.\n\nStep 1: Imports\nLet’s start by importing numpy (will be used later) and random.\n\n\n\n\n\n\nImports\n\n\n\n\n\nimport numpy as np\nimport random\n\n\n\n\n\nStep 2: Cards and scores\nDefine a Game class that is initialized with these class attributes:\n\n3-card deck array with cards 0, 1, and 2\nCard placeholders for each player in a cards array that are initialized as None\nScore placeholders for each player in scores array that are initialized as 0\n\n\n\n\n\n\n\nInitialize Game class\n\n\n\n\n\nclass Game:\n    def __init__(self):\n        self.deck = [0, 1, 2]\n        self.cards = [None, None]\n        self.scores = [0, 0]\n\n\n\n\n\nStep 3: Deal cards\nLet’s now add a deal_cards function into the Game class that samples two cards from the deck into self.cards. This is the first of two class methods.\n\n\n\n\n\n\ndeal_cards function\n\n\n\n\n\n    def deal_cards(self):\n        self.cards = random.sample(self.deck, 2)\n\n\n\n\n\nStep 4: Compare cards\nAccumulate the scores for each player using self.scores in a compare_cards function. This is the second class method.\n\n\n\n\n\n\nplay_round function\n\n\n\n\n\n    def compare_cards(self):\n        if self.cards[0] &gt; self.cards[1]:\n            self.scores[0] += 1\n            self.scores[1] -= 1\n        elif self.cards[0] &lt; self.cards[1]:\n            self.scores[0] -= 1\n            self.scores[1] += 1\n\n\n\n\n\nStep 5: Play round\nCreate a play_round function in the Game class that deals the cards using self.deal_cards.\n\n\n\n\n\n\nUpdate play_round function\n\n\n\n\n\n    def play_round(self):\n        self.deal_cards()\n        self.compare_cards()\n\n\n\n\n\nStep 6: Run the game\nCreate the main function that defines num_games as 100, creates an object of the Game class in game, and runs the game num_games times.\n\n\n\n\n\n\nmain function\n\n\n\n\n\ndef main():\n    num_games = 100\n    game = Game()\n\n    for _ in range(num_games):\n        game.play_round()\nThe game object now has all the attributes and methods defined in the Game class.\n\n\n\n\n\nStep 7: Print the scores\nPrint the scores for each player after the game ends, also in the main function.\n\n\n\n\n\n\nPrint scores\n\n\n\n\n\n\n    print(f\"After {num_games} games:\")\n    print(f\"Player 1 score: {game.scores[0]}\")\n    print(f\"Player 2 score: {game.scores[1]}\")\n\n\n\n\n\nStep 8: Create the main block\n\n\n\n\n\n\nMain block\n\n\n\n\n\nif __name__ == \"__main__\":\n    main()\n\n\n\n\n\nPutting it all together\n\n\n\n\n\n\nAll the code\n\n\n\n\n\nimport random\n\nclass Game:\n    def __init__(self):\n        self.deck = [0, 1, 2]\n        self.cards = [None, None]\n        self.scores = [0, 0]\n\n    def deal_cards(self):\n        self.cards = random.sample(self.deck, 2)\n\n    def compare_cards(self): \n        if self.cards[0] &gt; self.cards[1]:\n            self.scores[0] += 1\n            self.scores[1] -= 1\n        elif self.cards[0] &lt; self.cards[1]:\n            self.scores[0] -= 1\n            self.scores[1] += 1\n\n    def play_round(self):\n        self.deal_cards()\n        self.compare_cards()\n\ndef main():\n    num_games = 100\n    game = Game()\n\n    for _ in range(num_games):\n        game.play_round()\n\n    print(f\"After {num_games} games:\")\n    print(f\"Player 1 score: {game.scores[0]}\")\n    print(f\"Player 2 score: {game.scores[1]}\")\n\nif __name__ == \"__main__\":\n    main()\n\n\n\n\n\nRun the code\npython3 c2c.py\nThe output should look something like:\n\n\n\n\n\n\nSample code output from 100 games\n\n\n\n\n\nAfter 100 games:\nPlayer 1 score: -12\nPlayer 2 score: 12",
    "crumbs": [
      "About",
      "Setup and Face-up Poker"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "C2C: About",
    "section": "",
    "text": "We’re excited about learning through games. We developed Cards to Code as a foundational course on building a functional poker bot after running an AI Poker Camp course in San Francisco in summer 2024.\nCheck out Overbet.ai (under development) and Poker Camp.\nWe’ll also have more advanced courses available soon."
  },
  {
    "objectID": "about.html#about-us",
    "href": "about.html#about-us",
    "title": "C2C: About",
    "section": "",
    "text": "We’re excited about learning through games. We developed Cards to Code as a foundational course on building a functional poker bot after running an AI Poker Camp course in San Francisco in summer 2024.\nCheck out Overbet.ai (under development) and Poker Camp.\nWe’ll also have more advanced courses available soon."
  },
  {
    "objectID": "about.html#why-poker",
    "href": "about.html#why-poker",
    "title": "C2C: About",
    "section": "Why poker?",
    "text": "Why poker?\nLet’s split this into two questions.\n\nWhy play poker?\nGames of incomplete information like poker are most similar to real-life settings. Incomplete information means that there is some hidden information – in poker this is the private opponent cards and, in games like Texas Hold’em, the yet to be revealed board cards.\nThis means we need to infer our opponents’ hands and understand the probabilities of the cards that we can’t see.\nPoker is interesting from a math perspective – probability, expected value, risk, and bankroll management are all important skills.\nAlso from a psychological perspective – emotional control, reading opponents, and adapting to other players are all valuable.\nWe learn to make decisions and focus on the quality of the decision, rather than the results.\n\n\nWhy build a poker bot?\nPoker has clear rules and structure, so we can apply reinforcement learning where the rewards are the profits in the game. Reinforcement learning, game theory, and Monte Carlo methods are applicable across many domains.\nHere we mainly focus on a simplified poker game that can be solved in under a minute, but the same core principles apply for scaled up versions that are much larger and more complex, though they will also involve approximating states of the game.\nThe richness of the game is an ideal testbed for decision making under uncertainty, probabilistic reasoning, and taking actions that can have both immediate and longer horizon consequences.\nWe can use the bot solutions to gain insights from the underlying game itself. For example, you’ll see that bots automatically learn to bluff, showing that this is a theoretically correct play and not a “loss leader” just to get more action later.\nBeyond building a basic bot, there is a broad area for further research, including agent evaluation, opponent modeling, building an LLM model on top of the traditional poker agent, and intepretability of the agent."
  },
  {
    "objectID": "about.html#modern-ai-agents",
    "href": "about.html#modern-ai-agents",
    "title": "C2C: About",
    "section": "Modern AI agents",
    "text": "Modern AI agents\nLLM-based AI agents are very popular in 2024. The poker bot we’re building here is different and uses the classic AI method of reinforcement learning, where the goal is to maximize long-term reward given feedback from the environment (i.e. maximizing winnings in a poker game).\nThe poker bot learns how to make optimal decisions given a state of the game by repeatedly playing the game against itself and optimizing the strategy over time.\nLLM agents generally use large amounts of text data to understand and generate human-like language based on patterns in the data, which could include strategy in a game like poker.\nSo is RL still relevant? Yes! OpenAI recently released their o1 chain of thought model:\n\nOur large-scale reinforcement learning algorithm teaches the model how to think productively using its chain of thought in a highly data-efficient training process.\n\nIn chain of thought, each step in the reasoning process can be seen as a state with possible actions. RLHF (Reinforcement Learning from Human Feedback) is also a popular technique for LLM agents where a human acts as the reward signal to shape a model’s outputs.\nHybrid approaches seem promising and may be a future direction for poker agents as well."
  },
  {
    "objectID": "about.html#inspiration",
    "href": "about.html#inspiration",
    "title": "C2C: About",
    "section": "Inspiration",
    "text": "Inspiration\nSome inspiration for this course:\n\nNand to Tetris: Building a Modern Computer From First Principles\nNeural Networks: Zero to Hero: A course by Andrej Karpathy on building neural networks, from scratch, in code\nfast.ai’s Practical Deep Learning: A free course designed for people with some coding experience, who want to learn how to apply deep learning and machine learning to practical problems"
  },
  {
    "objectID": "3_gametrees.html",
    "href": "3_gametrees.html",
    "title": "C2C #3: Game Trees",
    "section": "",
    "text": "The standard way to solve a game tree like this is using backward induction, whereby we start with the leaves (i.e. the payoff nodes at the bottom) of the tree and see which decisions the last player, Player 2 in this case, will make at her decision nodes.\nPlayer 2’s goal is to minimize the maximum payoff of Player 1, which in the zero-sum setting is equivalent to minimizing her own maximum loss or maximizing her own minimum payoff. This is equivalent to a Nash equilibrium in the zero-sum setting.\nShe picks the right node on the left side (payoff -1 instead of -5) and the left node on the right side (payoff 3 instead of -6).\nThese values are then propagated up the tree so from Player 1’s perspective, the value of going left is 1 and of going right is -3. The other leaf nodes are not considered because Player 2 will never choose those. Player 1 then decides to play left to maximize his payoff.\n\nWe can see all possible payouts, where the rows are P1 actions and the columns are P2 actions after P1 actions (e.g. Left/Left means P1 chose Left and then P2 also chose Left).\n\n\n\nP1/P2\nLeft/Left\nLeft/Right\nRight/Left\nRight/Right\n\n\n\n\nLeft\n5,-5\n5,-5\n1,-1\n1,-1\n\n\nRight\n-3,3\n-3,3\n6,-6\n6,-6\n\n\n\nNote that Player 1 choosing right could result in a higher payout (6) if Player 2 also chose right, but a rational Player 2 would not do that, and so the algorithm requires maximizing one’s minimum payoff, which means Player 1 must choose left (earning a guaranteed value of 1).\nBy working backwards from the end of a game, we can evaluate each possible sequence of moves and propagate the values up the game tree.",
    "crumbs": [
      "About",
      "#3: Game Trees"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Cards to Code",
    "section": "",
    "text": "A course by Max Chiswick and Ross Rheingans-Yoo on building a basic poker bot from scratch.\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe course starts with game theory, then introduces a simplified poker setting, then the elements of the Counterfactual Regret Minimization (CFR) algorithm, and then beyond the vanilla CFR algorithm.\nThroughout the course, we’re building up code to first develop the game, and then piece by piece to develop the poker bot.\nWe end on opponent modeling and challenge problems that go beyond game theory optimal play.\nPrerequisites: Basic Python and math (no poker knowledge necessary)\nGroups/Questions: We encourage you to work through this in groups. You can chat in our Discord to discuss the course or look for others to work with.\nGet started now! Start with the Setup and Face-up Poker page.",
    "crumbs": [
      "About",
      "Home"
    ]
  },
  {
    "objectID": "1_gametheory.html",
    "href": "1_gametheory.html",
    "title": "C2C #1: Game Theory",
    "section": "",
    "text": "The most simple type of game is the one that we built in the setup section. Cards get flipped over and you either win or lose. Not much skill…and not much fun!\nOther games like Bingo, Dreidel, and of course Snakes and Ladders can deceptively seem like you have strategic control of the game, but really this is just the “illusion of agency”, since you’re only, for example, rolling dice to randomize your next move. A disturbing fact:\n\nThere are three main classes of games that do have significant strategic agency: perfect information, perfect information with randomness, and imperfect (hidden) information.\nWe can also think about games along the axis of fixed or random opponents and adversarial opponents.\n(There are also other game classes like sequential vs. simultaneous action games.)\n\n\n\n\n\n\n\n\nGame/Opponent\nFixed/Random Opponent\nAdversarial Opponent\n\n\n\n\nNo Player Agency/Decisions\nCandy Land, War, Dreidel, Bingo, Snakes and Ladders\nBlackjack dealer\n\n\nPerfect Info\nPuzzles, Rubix cube\nTictactoe, Checkers, Chess, Arimaa, Go, Connect Four\n\n\nPerfect Info with Randomness\nMonopoly, Risk\nBackgammon\n\n\nImperfect Info\nWordle, Blackjack\nPoker, Rock Paper Scissors, Figgie, StarCraft\n\n\n\n\n\nPerfect information games like Chess and Go are complex and have been the focus of recent AI research, much like poker. Solving these games can theoretically be done using backward induction, which means starting from possible end positions and working backwards.\n\n\n\nHere we want to primarily focus on the bottom right of this chart: imperfect information games with an adversarial opponent.\nThese games cannot be solved in the same way as perfect information games because we don’t know what state of the game we are in (we’ll look at this more in Game Trees).\nImperfect information games are representative of real world situations where information is usually incomplete, e.g. job interviews, investments, dating, and negotiations.\n\n\n\nThe goal of a game is to maximize the “utility”, which is usually a score or money or some kind of value.\nThere are two fundamental strategies:\n\nExploiting opponent weaknesses: Capitalize on specific flaws or tendencies of opponents, while putting yourself at risk if the opponent adapts well.\nBeing unexploitable: Playing a balanced, theoretically sound strategy, also known as “game theory optimal”. This does not maximize against weaker opponents, but rather aims to minimize losses against any possible opponents.",
    "crumbs": [
      "About",
      "#1: Game Theory"
    ]
  },
  {
    "objectID": "1_gametheory.html#types-of-games",
    "href": "1_gametheory.html#types-of-games",
    "title": "C2C #1: Game Theory",
    "section": "",
    "text": "The most simple type of game is the one that we built in the setup section. Cards get flipped over and you either win or lose. Not much skill…and not much fun!\nOther games like Bingo, Dreidel, and of course Snakes and Ladders can deceptively seem like you have strategic control of the game, but really this is just the “illusion of agency”, since you’re only, for example, rolling dice to randomize your next move. A disturbing fact:\n\nThere are three main classes of games that do have significant strategic agency: perfect information, perfect information with randomness, and imperfect (hidden) information.\nWe can also think about games along the axis of fixed or random opponents and adversarial opponents.\n(There are also other game classes like sequential vs. simultaneous action games.)\n\n\n\n\n\n\n\n\nGame/Opponent\nFixed/Random Opponent\nAdversarial Opponent\n\n\n\n\nNo Player Agency/Decisions\nCandy Land, War, Dreidel, Bingo, Snakes and Ladders\nBlackjack dealer\n\n\nPerfect Info\nPuzzles, Rubix cube\nTictactoe, Checkers, Chess, Arimaa, Go, Connect Four\n\n\nPerfect Info with Randomness\nMonopoly, Risk\nBackgammon\n\n\nImperfect Info\nWordle, Blackjack\nPoker, Rock Paper Scissors, Figgie, StarCraft\n\n\n\n\n\nPerfect information games like Chess and Go are complex and have been the focus of recent AI research, much like poker. Solving these games can theoretically be done using backward induction, which means starting from possible end positions and working backwards.\n\n\n\nHere we want to primarily focus on the bottom right of this chart: imperfect information games with an adversarial opponent.\nThese games cannot be solved in the same way as perfect information games because we don’t know what state of the game we are in (we’ll look at this more in Game Trees).\nImperfect information games are representative of real world situations where information is usually incomplete, e.g. job interviews, investments, dating, and negotiations.\n\n\n\nThe goal of a game is to maximize the “utility”, which is usually a score or money or some kind of value.\nThere are two fundamental strategies:\n\nExploiting opponent weaknesses: Capitalize on specific flaws or tendencies of opponents, while putting yourself at risk if the opponent adapts well.\nBeing unexploitable: Playing a balanced, theoretically sound strategy, also known as “game theory optimal”. This does not maximize against weaker opponents, but rather aims to minimize losses against any possible opponents.",
    "crumbs": [
      "About",
      "#1: Game Theory"
    ]
  },
  {
    "objectID": "1_gametheory.html#rock-paper-scissors",
    "href": "1_gametheory.html#rock-paper-scissors",
    "title": "C2C #1: Game Theory",
    "section": "Rock Paper Scissors",
    "text": "Rock Paper Scissors\n\n\n\nImage by Enzoklop under CC BY-SA 3.0\n\n\n\nRock defeats scissors, scissors defeats paper, and paper defeats rock\nYou get +1 point for a win, -1 for a loss, and 0 for ties\n\nThis simple game was the subject of a DeepMind paper in 2023, where they wrote:\n\nIn sequential decision-making, agent evaluation has largely been restricted to few interactions against experts, with the aim to reach some desired level of performance (e.g. beating a human professional player). We propose a benchmark for multiagent learning based on repeated play of the simple game Rock, Paper, Scissors.\n\nLet’s dig in to why RPS is a more interesting game than it might seem.\n\nPayoff Matrix\nThe core features of a game are its players, the actions of each player, and the payoffs. We can show this for RPS in the below payoff matrix, also known as normal-form.\n\n\n\nPlayer 1/2\nRock\nPaper\nScissors\n\n\n\n\nRock\n(0, 0)\n(-1, 1)\n(1, -1)\n\n\nPaper\n(1, -1)\n(0, 0)\n(-1, 1)\n\n\nScissors\n(-1, 1)\n(1, -1)\n(0, 0)\n\n\n\nThe payoffs for Player 1 are on the left and for Player 2 on the right in each payoff outcome of the game. For example, the bottom left payoff is when Player 1 plays Scissors and Player 2 plays Rock, resulting in -1 for P1 and +1 for P2.\n\n\nExpected Value\nA strategy says which actions you would take for every state of the game.\nExpected value represents the average outcome of a strategy if it were repeated many times. It’s calculated by multiplying each possible outcome by its probability of occurrence and then summing these products.\nSuppose that Player 1 plays the strategy:\n\n\\begin{cases}\nr = 0.5 \\\\\np = 0.25 \\\\\ns = 0.25\n\\end{cases}\n\nand Player 2 plays the strategy:\n\n\\begin{cases}\nr = 0.1 \\\\\np = 0.3 \\\\\ns = 0.6\n\\end{cases}\n\nLet’s add these to the matrix:\n\n\n\n\n\n\n\n\n\nPlayer 1/2\nRock (r_2=0.1)\nPaper (p_2=0.3)\nScissors (s_2=0.6)\n\n\n\n\nRock (r_1=0.5)\n(0, 0)\n(-1, 1)\n(1, -1)\n\n\nPaper (p_1=0.25)\n(1, -1)\n(0, 0)\n(-1, 1)\n\n\nScissors (s_1=0.25)\n(-1, 1)\n(1, -1)\n(0, 0)\n\n\n\nTo simplify, let’s just write the payoffs for Player 1 since payoffs for Player 2 will simply be the inverse:\n\n\n\n\n\n\n\n\n\nPlayer 1/2\nRock (r_2=0.1)\nPaper (p_2=0.3)\nScissors (s_2=0.6)\n\n\n\n\nRock (r_1=0.5)\n0\n-1\n1\n\n\nPaper (p_1=0.25)\n1\n0\n-1\n\n\nScissors (s_1=0.25)\n-1\n1\n0\n\n\n\nNow we can multiply the player action strategies together to get a percentage occurrence for each payoff in the matrix:\n\n\n\n\n\n\n\n\n\nPlayer 1/2\nRock (r_2=0.1)\nPaper (p_2=0.3)\nScissors (s_2=0.6)\n\n\n\n\nRock (r_1=0.5)\nVal: 0 Pr: 0.5(0.1) = 0.05\nVal: -1 Pr: 0.5(0.3) = 0.15\nVal: 1 Pr: 0.5(0.6) = 0.3\n\n\nPaper (p_1=0.25)\nVal: 1 Pr: 0.25(0.1) = 0.025\nVal: 0 Pr: 0.25(0.3) = 0.075\nVal: -1 Pr: 0.25(0.6) = .15\n\n\nScissors (s_1=0.25)\nVal: -1 Pr: 0.25(0.1) = 0.025\nVal: 1 Pr: 0.25(0.3) = 0.075\nVal: 0 Pr: 0.25(0.6) = .15\n\n\n\nNote that the total probabilities sum to 1 and each row and column sums to the probability of playing that row or column.\nWe can work out the expected value of the game to Player 1 (summing all payoffs multiplied by probabilities from top left to bottom right):\n\\mathbb{E} = 0(0.05) + -1(0.15) + 1(0.3) + 1(0.025) + 0(0.075) + -1(0.15) + -1(0.025) + 1(0.075) + 0(0.15) = 0.075\nTherefore P1 is expected to gain 0.075 per game given these strategies. Since payoffs are reversed for P2, P2’s expectation is -0.075 per game.\n\n\nZero-sum\nWe see in the matrix that every payoff is zero-sum, i.e. the sum of the payoffs to both players is 0. This means the game is one of pure competition. Any amount P1 wins is from P2 and vice versa.\n\n\nNash equilibrium\nA Nash equilibrium means that no player can improve their expected payoff by unilaterally changing their strategy. That is, changing one’s strategy can only result in the same or worse payoff (assuming the other player does not change).\nIn RPS, the Nash equilibrium strategy is to play each action r = p = s = 1/3 of the time. I.e., to play totally randomly.\nThis is called a mixed strategy, as opposed to a pure strategy, which would select only one action. Mixed strategies are useful in games of imperfect information because it’s valuable to not be predictable and to conceal your private information. In perfect information games, the theoretically optimal play would not contain any mixing (i.e., if you could calculate all possible moves to the end of the game).\nThe equilibrium RPS strategy is worked out below:\n\n\n\n\n\n\nNash equilibrium strategy for RPS\n\n\n\n\n\n\n\n\nPlayer 1/2\nRock\nPaper\nScissors\n\n\n\n\nRock (r)\n(0, 0)\n(-1, 1)\n(1, -1)\n\n\nPaper (p)\n(1, -1)\n(0, 0)\n(-1, 1)\n\n\nScissors (s)\n(-1, 1)\n(1, -1)\n(0, 0)\n\n\n\nIf Player 1 plays Rock with probability r, Paper with probability p, and Scissors with probability s, we have the following expected value equations for Player 2:\n\\mathbb{E_2}(\\text{R}) = -1p + 1s\n\\mathbb{E_2}(\\text{P}) = 1r - 1s\n\\mathbb{E_2}(\\text{S}) = -1r + 1p\n(To see each of these, sum the payoffs for each column with P2 payoffs and P1 probabilities.)\nSince no action dominates, we know that the EV of every strategic action should be equal (since if a certain strategy was best, we’d want to always play that strategy).\nTo solve for r, p, and s, we can start by setting these EVs equal:\n\\mathbb{E_2}(\\text{R}) = \\mathbb{E_2}(\\text{P})\n-1p + 1s = 1r - 1s\n2s = p + r\nThen setting these equal:\n\\mathbb{E_2}(\\text{R}) = \\mathbb{E_2}(\\text{S})\n-1p + 1s = -1r + 1p\ns + r = 2p\nAnd finally setting these equal:\n\\mathbb{E_2}(\\text{P}) = \\mathbb{E_2}(\\text{S})\n1r - 1s = -1r + 1p\n2r = s + p\nNow we have these equations:\n\n\\begin{cases}\n2s = p + r \\\\\ns + r = 2p \\\\\n2r = s + p\n\\end{cases}\n\nWe can rewrite the 1st:\nr = 2s - p\nAnd combine with the 2nd:\ns + (2s - p) = 2p\n3s = 3p\nResulting in:\ns = p\nNow we can go back to the 2nd equation:\ns + r = 2p\nAnd insert s = p:\ns + r = 2s\nAnd arrive at:\nr = s\nWe now see that all are equal:\ns = p = r\nWe also know that they must all sum to 1:\nr + p + s = 1\nSince they’re all equal and sum to 1, we can substitute p and s with r:\n3r = 1\nr = 1/3\nSo all actions are taken with probability 1/3:\nr = p = s = 1/3 \\quad \\blacksquare\nBy symmetry, the same equilibrium strategy is true for Player 2.\n\n\n\nPlaying this strategy means that whatever your opponent does, you will breakeven! For example, think about an opponent that always plays Rock.\n\n\\begin{equation}\n\\begin{split}\n\\mathbb{E}(\\text{Equilibrium vs. Rock}) &= 0(r) + 1(p) + -1(s) \\\\\n&= 0(1/3) + 1(1/3) + -1(1/3) \\\\\n&= 0\n\\end{split}\n\\end{equation}\n\nHow about the case of the opponent playing 60% Rock, 20% Paper, 20% Scissors?\n\\mathbb{E}(\\text{Equilibrium vs. 622}) = 0.6(\\frac{1}{3}) + 1(p) + -1(s)\n\n\nExploiting vs. Nash\nThe equilibrium strategy vs. a pure Rock opponent is a useful illustration of the limitations of playing at equilibrium. The Rock opponent is playing the worst possible strategy, yet equilibrium is still breaking even.",
    "crumbs": [
      "About",
      "#1: Game Theory"
    ]
  },
  {
    "objectID": "1_gametheory.html#soccer-kicker-vs.-goalie",
    "href": "1_gametheory.html#soccer-kicker-vs.-goalie",
    "title": "C2C #1: Game Theory",
    "section": "Soccer Kicker vs. Goalie",
    "text": "Soccer Kicker vs. Goalie\n\nDeviating from Nash\nBest response to each other in equilibrium, no incentive to deviate. Deviate means no longer in equilibrium, though EV is same. only guarantee worst-case payoff of 0 if you randomize\n\n\nIndifference\nwhy is fundamental goal to make other player indifferent? if deviate, indifference guarantees can’t improve by changing unliterally",
    "crumbs": [
      "About",
      "#1: Game Theory"
    ]
  },
  {
    "objectID": "1_gametheory.html#clairvoyance-poker",
    "href": "1_gametheory.html#clairvoyance-poker",
    "title": "C2C #1: Game Theory",
    "section": "Clairvoyance Poker",
    "text": "Clairvoyance Poker\nBack to poker. We previously built Face-up 3-card Poker where each player was dealt a card from the deck of {0, 1, 2} and the high card won.\nNow we’re going to make the game more interesting.\n\nPlayer 1 will always be dealt card 1.\nPlayer 2 will be dealt randomly from {0, 2}.\nP1 knows P2 has either card 0 or 2, but not which. P2 knows that P1 has card 1.\n\nLet’s modify the code:\n\n\n\n\n\n\nUpdate deal_cards function\n\n\n\n\n\n    def deal_cards(self):\n        self.cards[0] = 1\n        remaining_cards = [card for card in self.deck if card != self.cards[0]]\n        self.cards[1] = random.choice(remaining_cards)\n\n\n\n\nEquity in Poker\nshow same equity\n\n\nBetting Rules and Sequences\n\nBoth players ante 1 chip, so the starting pot is 2\nPlayer 1 acts first and can either Bet 1 or Check (Pass)\n\nIf P1 Bets, P2 can Call or Fold\nIf P1 Checks, P2 can Bet or Check\n\nIf P1 Passes and P2 Bets, P1 can Call or Fold\n\n\n\nThere are three possible ways for the hand to end:\n\nCheck Check: Pot 2, hands go to showdown and player with highest card wins 1 (the ante)\nBet Call: Pot 4, hands go to showdown and player with highest card wins 2 (the ante + bet)\nBet Fold: Pot 3, player who bets wins 1 (the ante)\n\n–&gt; Add pot size stuff\n\n\nStarting Strategies\nLet’s hard-code some initial strategies for both players.\n\nP1: Randomly bet/check\nP2 after bet: Call with card 2 and fold with card 0\nP2 after check: Bet with card 2 and randomize between bet/check with card 0\n\nThis strategy randomizes uncertain decisions and takes clearly smart decisions when possible.\nWhat does that mean?\n\nP2 call with card 2 after P1 bet: This is the best hand and guarantees a win\nP2 fold with card 0 after P1 bet: This is the worst card and can’t win\nP2 bet with card 2 after P1 check: This is the best card and betting can only possibly win additional chips\n\n—&gt; show wins/losses, frequency, total for each strategic decision P1 bet, P1 check/call, P1 check/fold P2 0 after bet call, P2 0 after bet fold, P2 0 after check bet, P2 0 after check check P2 2 after bet call, P2 2 after bet fold, P2 2 after check bet, P2 2 after check check\n\n\nDominated Strategy\nNote that P1 betting is strictly worse and why\n\n\nYour Strategies\n—&gt; Try strat for P1 then for P2 See results\n\n\nIndifference in Poker\nFiguring out strategy for 0 facing check, actually thinking about strategy for 0 and 2. goal indifference\nEV folding 0, EV calling should also be 0\nshows EV = bluff3 - value1 (but this is for overall range 1/4 bluffs)\nbluff to value ratio bet/(bet+pot)\nFiguring out strategy for 1 after check/bet\nbluffing frequency goes up as bet size gets larger relative to pot\nOnly mix when both same EV\ncall strategy wants indifferent between betting and checking card 0 checking 0 has EV 0 betting 0 also needs to have EV 0\nEV = fold2 - call1 get call = 2/3 1 - bet/(bet+pot) larger bet means call less, let them get away with bluffs the bigger they bet\n\n\nEquity vs. Expected Value\ncan tell opponent strategy to bet 1/3 0s but can’t stop call more catches more bluffs, but pays off more value bets\nSometimes have to bluff with bad cards because EV higher than if you always folded with bad cards.\nAdd uncertainty the opponent has to think about in counter-strategy If always folded bad cards, opponent knows you keep playing with good #\n\n\nBonus: Ideal Bet Size",
    "crumbs": [
      "About",
      "#1: Game Theory"
    ]
  }
]