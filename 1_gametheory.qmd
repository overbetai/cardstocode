---
title: "C2C #1: Game Theory"
---
## Types of games
The most simple type of game is the one that we built in the [setup](setup.qmd) section. Cards get flipped over and you either win or lose. Not much skill...and not much fun! 

Other games like Bingo, Dreidel, and of course Snakes and Ladders can deceptively seem like you have strategic control of the game, but really this is just the "illusion of agency", since you're only, for example, rolling dice to randomize your next move. A disturbing fact: 

![](assets/eliezer_snakes.jpg) 

There are three main classes of games that do have significant strategic agency: **perfect information**, **perfect information with randomness**, and **imperfect (hidden) information**. 

We can also think about games along the axis of **fixed or random opponents** and **adversarial opponents**. 

(There are also other game classes like sequential vs. simultaneous action games.) 

| Game/Opponent     | Fixed/Random Opponent | Adversarial Opponent | 
|------------|-----------|--------|
| **No Player Agency/Decisions**      | Candy Land, War, Dreidel, Bingo, Snakes and Ladders, Slot machine          | Blackjack dealer
| **Perfect Info**     | Puzzles, Rubix cube  | Tictactoe, Checkers, Chess, Arimaa, Go, Connect Four |
| **Perfect Info with Randomness**     |   | Backgammon |
| **Imperfect Info**    | Wordle, Blackjack  | Poker, Rock Paper Scissors, Figgie, StarCraft |

### Perfect information games
Perfect information games like Chess and Go are complex and have been the focus of recent AI research, much like poker. Solving these games can theoretically be done using **backward induction**, which means starting from possible end positions and working backwards. 

### Imperfect information games
Here we want to primarily focus on the bottom right of this chart: imperfect information games with an adversarial opponent. 

These games cannot be solved in the same way as perfect information games because we don't know what state of the game we are in (we'll look at this more in [Game Trees](3_gametrees.qmd)). 

Imperfect information games are representative of real world situations where information is usually incomplete, e.g. job interviews, investments, dating, and negotiations.

### Winning games
The goal of a game is to maximize the "utility", which is usually a score or money or some kind of value. 

There are two fundamental strategies: 

1. **Exploiting opponent weaknesses:** Capitalize on specific flaws or tendencies of opponents, while putting yourself at risk if the opponent adapts well

2. **Being unexploitable:** Playing a balanced, theoretically sound strategy, also known as "game theory optimal". This does not maximize against weaker opponents, but rather aims to minimize losses against any possible opponents. 

## Rock Paper Scissors
![Image by [Enzoklop](https://en.wikipedia.org/wiki/Rock_paper_scissors#/media/File:Rock-paper-scissors.svg) under [CC BY-SA 3.0](https://creativecommons.org/licenses/by-sa/3.0/)](rps.png){width=50%}

- Rock defeats scissors, scissors defeats paper, and paper defeats rock
- You get +1 point for a win, -1 for a loss, and 0 for ties

This simple game was the subject of a DeepMind paper in 2023, where they wrote: 

> In sequential decision-making, agent evaluation has largely been restricted to few interactions against experts, with the aim to reach some desired level of performance (e.g. beating a human professional player). We propose a benchmark for multiagent learning based on repeated play of the simple game Rock, Paper, Scissors.

Let's dig into why RPS is a more interesting game than it might seem. 

### Payoff Matrix
The core features of a game are its players, the actions of each player, and the payoffs. We can show this for RPS in the below payoff matrix, also known as normal-form.

| Player 1/2 | Rock    | Paper   | Scissors |
|--------|---------|---------|----------|
| Rock   | (0, 0)  | (-1, 1) | (1, -1)  |
| Paper  | (1, -1) | (0, 0)  | (-1, 1)  |
|Scissors| (-1, 1) | (1, -1) | (0, 0)   |

The payoffs for Player 1 are on the left and for Player 2 on the right in each payoff outcome of the game. For example, the bottom left payoff is when Player 1 plays Scissors and Player 2 plays Rock, resulting in -1 for P1 and +1 for P2. 

### Expected Value
A strategy says which actions you would take for every state of the game.

Expected value represents the average outcome of a strategy if it were repeated many times. Itâ€™s calculated by multiplying each possible outcome by its probability of occurrence and then summing these products.

Suppose that  Player 1 plays the strategy: 

$$
\begin{cases}
r = 0.5 \\
p = 0.25 \\
s = 0.25
\end{cases}
$$

and Player 2 plays the strategy: 

$$
\begin{cases}
r = 0.1 \\
p = 0.3 \\
s = 0.6
\end{cases}
$$

Let's add these to the matrix: 

| Player 1/2 | Rock ($r=0.1$)   | Paper ($p=0.3$)  | Scissors ($s=0.6$) |
|--------|---------|---------|----------|
| Rock ($r=0.5$)   | (0, 0)  | (-1, 1) | (1, -1)  |
| Paper ($p=0.25$) | (1, -1) | (0, 0)  | (-1, 1)  |
| Scissors ($s=0.25$) | (-1, 1) | (1, -1) | (0, 0)   |

To simplify, let's just write the payoffs for Player 1 since payoffs for Player 2 will simply be the inverse: 

| Player 1/2 | Rock ($r=0.1$)   | Paper ($p=0.3$)  | Scissors ($s=0.6$) |
|--------|---------|---------|----------|
| Rock ($r=0.5$)   | 0  | -1 | 1 |
| Paper ($p=0.25$) | 1 | 0  | -1 |
| Scissors ($s=0.25$) | -1 | 1 | 0 |

Now we can multiply the player action strategies together to get a percentage occurrence for each payoff in the matrix: 

| Player 1/2 | Rock ($r_2=0.1$)   | Paper ($p_2=0.3$)  | Scissors ($s_2=0.6$) |
|--------|---------|---------|----------|
| Rock ($r_1=0.5$)   | 0 ($0.5(0.1) = 0.05$) | -1 ($0.5(0.3) = 0.15$) | 1 ($0.5(0.6) = 0.3$) |
| Paper ($p_1=0.25$) | 1 ($0.25(0.1) = 0.025$) | 0  ($0.25(0.3) = 0.075$) | -1 ($0.25(0.6) = .15$) |
| Scissors ($s_1=0.25$) | -1 ($0.25(0.1) = 0.025$) | 1 ($0.25(0.3) = 0.075$) | 0 ($0.25(0.6) = .15$) |

Note that the total probabilities sum to 1 and each row and column sums to the probability of playing that row or column. 

We can work out the expected value of the game to Player 1 (summing all payoffs multiplied by probabilities from top left to bottom right): 

$\mathbb{E} = 0(0.05) + -1(0.15) + 1(0.3) + 1(0.025) + 0(0.075) + -1(0.15) + -1(0.025) + 1(0.075) + 0(0.15) = 0.075$ 

Therefore P1 is *expected* to gain 0.075 per game given these strategies. Since payoffs are reversed for P2, P2's expectation is -0.075 per game. 

### Zero-sum
We see in the matrix that every payoff is zero-sum, i.e. the sum of the payoffs to both players is 0. This means the game is one of pure competition. Any amount P1 wins is from P2 and vice versa. 

### Nash equilibrium
A Nash equilibrium means that no player can improve their expected payoff by unilaterally changing their strategy. That is, changing one's strategy can only result in the same or worse payoff (assuming the other player does not change). 

In RPS, the Nash equilibrium strategy is to play each action $r = p = s = 1/3$ of the time. I.e., to play totally randomly. 

This is called a mixed strategy, as opposed to a pure strategy, which would select only one action. Mixed strategies are useful in games of imperfect information because it's valuable to not be predictable and to conceal your private information. In perfect information games, the theoretically optimal play would not contain any mixing (i.e. if you could calculate all possible moves to the end of the game). 

The equilibrium RPS strategy is worked out below: 

:::{.callout-tip collapse="true" appearance="minimal"}
## Nash equilibrium strategy for RPS
If Player 1 plays Rock with probability $r$, Paper with probability $p$, and Scissors with probability $s$, we have the following expected value equations for Player 2: 

$\mathbb{E}(\text{R}) = -1p + 1s$

$\mathbb{E}(\text{P}) = 1r - 1s$

$\mathbb{E}(\text{S}) = -1r + 1p$

Since no action dominates, we know that the EV of every strategic action should be equal  (since if a certain strategy was best, we'd want to always play that strategy). 

To solve for $r$, $p$, and $s$, we can start by setting these EVs equal: 

$\mathbb{E}(\text{R}) = \mathbb{E}(\text{P})$

$-1p + 1s = 1r - 1s$

$2s = p + r$

Then setting these equal: 

$\mathbb{E}(\text{R}) = \mathbb{E}(\text{S})$

$-1p + 1s = -1r + 1p$

$s + r = 2p$

And finally setting these equal: 

$\mathbb{E}(\text{P}) = \mathbb{E}(\text{S})$

$1r - 1s = -1r + 1p$

$2r = s + p$

Now we have these equations:  

$$
\begin{cases}
2s = p + r \\
s + r = 2p \\
2r = s + p
\end{cases}
$$

We can rewrite the 1st: 

$r = 2s - p$

And combine with the 2nd: 

$s + (2s - p) = 2p$

$3s = 3p$

Resulting in: 

$s = p$

Now we can go back to the 2nd equation: 

$s + r = 2p$

And insert $s$ = $p$: 

$s + r = 2s$

And arrive at: 

$r = s$

We now see that all are equal: 

$s = p = r$

We also know that they must all sum to $1$: 

$r + p + s = 1$

Since they're all equal and sum to $1$, we can substitute $p$ and $s$ with $r$: 

$3r = 1$

$r = 1/3$

So all actions are taken with probability $1/3$: 

$r = p = s = 1/3 \quad \blacksquare$
:::

Playing this strategy means that whatever your opponent does, you will breakeven! For example, think about an opponent that always plays Rock. 

$$
\begin{equation}
\begin{split}
\mathbb{E}(\text{Equilibrium vs. Rock}) &= 0(r) + 1(p) + -1(s) \\
&= 0(1/3) + 1(1/3) + -1(1/3) \\
&= 0
\end{split}
\end{equation}
$$

### Exploiting vs. Nash
The equilibrium strategy vs. a pure Rock opponent is a useful illustration of the limitations of playing at equilibrium. The Rock opponent is playing the worst possible strategy, yet equilibrium is still breaking even. 

## Soccer Kicker vs. Goalie

### Deviating from Nash

Best response to each other in equilibrium, no incentive to deviate. 
Deviate means no longer in equilibrium, though EV is same. 
only guarantee worst-case payoff of 0 if you randomize

### Indifference 
why is fundamental goal to make other player indifferent?
if deviate, 
indifference guarantees can't improve by changing unliterally 


## Clairvoyance Poker 
Back to poker. We built **Face-up 3-card Poker** where each player was dealt a card from the deck of {0, 1, 2} and the high card won. 

Now we're going to make the game more interesting. 

- Player 1 will *always* be dealt card 1. 

- Player 2 will be dealt randomly from {0, 2}. This card is not known to P1. 

- P1 knows P2 has either card 0 or 2, but not which. P2 knows that P1 has card 1. 

Let's modify the code: 

```python
    def deal_cards(self):
        self.cards[0] = 1
        remaining_cards = [card for card in self.deck if card != self.cards[0]]
        self.cards[1] = random.choice(remaining_cards)
```

### Equity in Poker


### Betting Rules and Sequences 
- Both players ante 1 chip, so the starting pot is 2

- Player 1 acts first and can either Bet 1 or Check (Pass)

    - If P1 Bets, P2 can Call or Fold

    - If P1 Checks, P2 can Bet or Check

        - If P1 Passes and P2 Bets, P1 can Call or Fold

There are three possible ways for the hand to end: 

- Check Check: Pot 2, hands go to *showdown* and player with highest card wins 1 (the ante)
- Bet Call: Pot 4, hands go to *showdown* and player with highest card wins 2 (the ante + bet)
- Bet Fold: Pot 3, player who bets wins 1 (the ante)

### Starting Strategies
Let's hard-code some initial strategies for both players. 

- P1: Randomly bet/check
- P2 after bet: Call with card 2 and fold with card 0
- P2 after check: Bet with card 2 and randomize between bet/check with card 0

This strategy randomizes uncertain decisions and takes clearly smart decisions when possible. 

Why? 

- P2 call with card 2 after P1 bet: This is the best hand and guarantees a win

- P2 fold with card 0 after P1 bet: This is the worst card and can't win

- P2 bet with card 2 after P1 check: This is the best card and betting can only possibly win additional chips

### Dominated Strategy 

### Indifference in Poker 

### Equity vs. Expected Value 


Sometimes have to bluff with bad cards because EV higher than if you 
always folded with bad cards. 

Add uncertainty the opponent has to think about in counter-strategy
If always folded bad cards, opponent knows you keep playing with good # 

